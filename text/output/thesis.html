<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta http-equiv="Content-Style-Type" content="text/css" />
        <meta name="generator" content="pandoc" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
                        <title>00_demo</title>
        <style type="text/css">code{white-space: pre;}</style>
                                            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
                            <style>
            body {
                font-family: Georgia;
                max-width: 800px;
                margin: 0 auto;
                line-height: 30px;
                font-size: 18px;
                padding-left: 350px;
                padding-right: 50px;
                color: #111;
            }
            
            h1, h2, h3, h4, h5, h6 {
                font-family: Arial;
            }
            
            h1 {
                padding-top: 200px;
                line-height: 50px;
            }
            h2 {
                padding-top: 30px;
            }
            h3 {
                padding-top: 20px;
            }
            h4 {
                padding-top: 10px;
            }
            p {
                text-align: justify;
            }
            p a {
                word-wrap: break-word;
                white-space: pre;
            }
            code {
                word-wrap: break-word;
            }
            blockquote {
                border-left: 3px solid #eee;
                margin-left: 20px;
                padding-left: 20px;
            }
            
            ::selection {
                background-color: #E4E4E4;
            }
            
            table {
                width: 100%;
            }
            table caption {
                font-weight: bold;
            }
            table tr {
                padding: 0;
                margin: 0;
                background-color: #f0f0f0;
            }
            table tr.even {
                background-color: #fafafa;
            }
            table td {
                margin: 0;
                padding: 3px 5px;
            }
            
            p span.added {
                color: green;
                background-color: #FFF3C5;
            }
            p span.removed {
                color: red;
                background-color: #FFF3C5;
            }
            
            #title-page {
                padding: 80px 0;
            }
            
            #TOC {
                position: fixed;
                left: 0;
                top: 0;
                overflow-y: scroll;
                height: 100%;
                background: #fafafa;
                max-width: 300px;
                font-family: Arial;
                font-size: 15px;
                line-height: 30px;
            }
            ::-webkit-scrollbar {
                width: 8px;
            }
            ::-webkit-scrollbar-track {
                background-color: #ECECEC;
            }
            ::-webkit-scrollbar-thumb {
                background-color: #B0B0B0;
                border-radius: 8px;
            }
            #TOC > ul {
                padding-right: 10px;
            }
            #TOC ul {
                list-style: none;
                padding-left: 20px;
            }
            #TOC ul li a {
                text-decoration: none;
                color: #364149;
                text-overflow: ellipsis;
                display: block;
                white-space: nowrap;
                overflow: hidden;
            }
            #TOC ul li a:hover {
                color: #008cff;
            }
            
            .figure {
                text-align: center;
            }
            .figure p {
                text-align: center;
                font-style: italic;
            }
            .figure img {
                  width: 100%;
            }
            </style>
                <script src="js/jquery.js"></script>
        <script src="js/diff.js"></script>
        <script src="js/main.js"></script>
    </head>
    <body>
                <!--
                -->
        <div id="title-page">
            <h1>Бейсови Модели за Дълбоко Подсилено Обучение<h1>
            <h2>Firstname Surname</h2>
        </div>
                    <div id="TOC">
                <ul>
                <li><a href="#обобщение"><span class="toc-section-number">1</span> Обобщение</a><ul>
                <li><a href="#цели-на-дисертацията"><span class="toc-section-number">1.1</span> Цели на дисертацията</a></li>
                <li><a href="#модел-на-агента"><span class="toc-section-number">1.2</span> Модел на агента</a><ul>
                <li><a href="#пространство-на-състоянието-state-space"><span class="toc-section-number">1.2.1</span> Пространство на състоянието (State space)</a></li>
                <li><a href="#пространство-на-действията-action-space"><span class="toc-section-number">1.2.2</span> Пространство на действията (Action space)</a></li>
                <li><a href="#модел-за-избор-на-действия"><span class="toc-section-number">1.2.3</span> Модел за избор на действия</a></li>
                <li><a href="#определяне-на-награди"><span class="toc-section-number">1.2.4</span> Определяне на награди</a></li>
                <li><a href="#оценка-на-модела-за-избор-на-действия"><span class="toc-section-number">1.2.5</span> Оценка на модела за избор на действия</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#мотивация"><span class="toc-section-number">2</span> Мотивация</a></li>
                <li><a href="#описание-на-работата-на-агента"><span class="toc-section-number">3</span> Описание на работата на агента</a><ul>
                <li><a href="#дадено-на-агента"><span class="toc-section-number">3.1</span> Дадено на агента</a></li>
                <li><a href="#задачи"><span class="toc-section-number">3.2</span> Задачи</a></li>
                </ul></li>
                <li><a href="#абстракт">Абстракт</a></li>
                <li><a href="#благодарности">Благодарности</a></li>
                <li><a href="#list-of-tables">List of tables</a></li>
                <li><a href="#речник">Речник</a></li>
                <li><a href="#увод"><span class="toc-section-number">4</span> Увод</a><ul>
                <li><a href="#структура-на-дисертационния-труд"><span class="toc-section-number">4.1</span> Структура на дисертационния труд</a></li>
                </ul></li>
                <li><a href="#литературен-обзор"><span class="toc-section-number">5</span> Литературен обзор</a><ul>
                <li><a href="#подсилено-обучение"><span class="toc-section-number">5.1</span> Подсилено обучение</a><ul>
                <li><a href="#дълбоко-обучение"><span class="toc-section-number">5.1.1</span> Дълбоко обучение</a></li>
                <li><a href="#дълбоко-подсилено-обучение"><span class="toc-section-number">5.1.2</span> Дълбоко подсилено обучение</a></li>
                </ul></li>
                <li><a href="#бейсова-статистика"><span class="toc-section-number">5.2</span> Бейсова статистика</a><ul>
                <li><a href="#монте-карло-алгоритми-за-марковски-вериги-mcmc"><span class="toc-section-number">5.2.1</span> Монте Карло алгоритми за Марковски Вериги (MCMC)</a></li>
                <li><a href="#извод-със-свободни-вариационни-параметри"><span class="toc-section-number">5.2.2</span> Извод със свободни вариационни параметри</a></li>
                <li><a href="#бейсови-невронни-мрежи"><span class="toc-section-number">5.2.3</span> Бейсови Невронни Мрежи</a></li>
                </ul></li>
                <li><a href="#автоматизирано-тестване-на-гпи"><span class="toc-section-number">5.3</span> Автоматизирано тестване на ГПИ</a><ul>
                <li><a href="#автоматизирано-тестване-на-android-приложения"><span class="toc-section-number">5.3.1</span> Автоматизирано тестване на Android приложения</a></li>
                <li><a href="#проверка-на-качеството"><span class="toc-section-number">5.3.2</span> Проверка на качеството</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#цел-и-задачи"><span class="toc-section-number">6</span> Цел и задачи</a></li>
                <li><a href="#среда-за-изучаване-rl-exploration-на-гпи-приложения"><span class="toc-section-number">7</span> Среда за изучаване (RL exploration) на ГПИ приложения</a><ul>
                <li><a href="#android-специфична-среда"><span class="toc-section-number">7.1</span> Android специфична среда</a></li>
                <li><a href="#web-специфична-среда"><span class="toc-section-number">7.2</span> Web специфична среда</a></li>
                </ul></li>
                <li><a href="#изучаване-на-визуални-среди-learning-to-explore-visual-environments"><span class="toc-section-number">8</span> Изучаване на визуални среди (Learning to explore visual environments)</a><ul>
                <li><a href="#related-work"><span class="toc-section-number">8.1</span> Related Work</a></li>
                <li><a href="#модел"><span class="toc-section-number">8.2</span> Модел</a></li>
                </ul></li>
                <li><a href="#експерименти-и-резултати"><span class="toc-section-number">9</span> Експерименти и резултати</a></li>
                <li><a href="#заключение"><span class="toc-section-number">10</span> Заключение</a><ul>
                <li><a href="#нерешени-проблеми"><span class="toc-section-number">10.1</span> Нерешени проблеми</a></li>
                <li><a href="#бъдеща-работа"><span class="toc-section-number">10.2</span> Бъдеща работа</a></li>
                <li><a href="#дискусия"><span class="toc-section-number">10.3</span> Дискусия</a></li>
                </ul></li>
                <li><a href="#приложение-1-някои-важни-вероятностни-разпределения">Приложение 1: Някои важни вероятностни разпределения</a></li>
                <li><a href="#приложение-2-фигури">Приложение 2: Фигури</a></li>
                <li><a href="#литература">Литература</a></li>
                </ul>
            </div>
                                <h1 id="обобщение"><span class="header-section-number">1</span> Обобщение</h1>
<p>Целта на настоящата работа е да създаде математически модел на самообучаващ се агент и да реализира агент, който напълно изследва дадена среда на Графичен Потребителски Интерфейс (ГПИ). Като входни данни за модела се използват изображения от ГПИ и информация за неговата сегментация. Агентът може да извършва действия, променяйки състоянието на ГПИ средата. Адекватността на извършените действия се оценява с награда, която средата предоставя. Наградата се определя от различни фактори, един от които е процентът покрит нов програмен код. Агентът успешно е изследвал средата, когато покритието на код на средата е пълно.</p>
<p>На базата на този модел ще създадем агент, който ще намира грешки в програмни продукти, ще синтезира пакети от автоматични тестове за качеството на софтуера и ще изпълнява посочени от потребителя задачи.</p>
<h2 id="цели-на-дисертацията"><span class="header-section-number">1.1</span> Цели на дисертацията</h2>
<p>Целта на дисертацията е да дефинира политика <span class="math inline">\(\pi\)</span>, определяща поведението на агента.</p>
<ul>
<li>Дефиниране на множество от възможни действия на агента (пространтство на действията)</li>
<li>Създаване на модел, който избира действията на агента</li>
<li>Създаване на среда (за тестване и използване на агента) в която ще работи агента
<ul>
<li>Подбиране на приложения (applications)</li>
<li>Измерване на покритието на код</li>
<li>Създаване на изображение и сегментация от текущото състояние на средата</li>
</ul></li>
<li>Избор на метрики за оценка на действията на агента</li>
<li>Предварително обучение (с учител) на агента с данни от хора, взаимодействащи със средата (imitation learning)</li>
<li>Провеждане на експерименти и анализ на постигнатите резултати</li>
</ul>
<h2 id="модел-на-агента"><span class="header-section-number">1.2</span> Модел на агента</h2>
<h3 id="пространство-на-състоянието-state-space"><span class="header-section-number">1.2.1</span> Пространство на състоянието (State space)</h3>
<p>Дадено състояние <span class="math inline">\(s\)</span> на средата съдържа цветно изображение <span class="math inline">\(I\)</span> и информация за сегментацията DOM модел <span class="math inline">\(D\)</span>, т.е. <span class="math inline">\(s = (I, D)\)</span>. Изображението има размер <span class="math inline">\(W \times H \times 3\)</span>, където <span class="math inline">\(W\)</span> е широчината на изображението в пиксели, <span class="math inline">\(H\)</span> височината на изображението в пиксели и 3 - броя на цветовете в палитрата (червено, зелено и синьо (rgb)). DOM моделът е представен като списък от текстови елементи, а информацията за сегментацията на изображението е дадена от наредената четворка <span class="math inline">\((x, y, w, h)\)</span> <span class="math inline">\(x\)</span> абцисната координата, <span class="math inline">\(y\)</span> ординатната ос, <span class="math inline">\(w\)</span> - широчината на сегмента, <span class="math inline">\(h\)</span> - височина на сегмента.</p>
<h3 id="пространство-на-действията-action-space"><span class="header-section-number">1.2.2</span> Пространство на действията (Action space)</h3>
<p>Позицията на курсора <span class="math inline">\(m = (m_x, m_y) \in [0, W) \times [0, H)\)</span> се моделира чрез мултиномиално разпределение върху възможните позиции. ГПИ средата не изисква наличие на клавиатура, защото . Възможните действия са: click, drag, scroll-up, scroll-down.</p>
<h3 id="модел-за-избор-на-действия"><span class="header-section-number">1.2.3</span> Модел за избор на действия</h3>
<p>Агентът избира действие <span class="math inline">\(a\)</span> във време <span class="math inline">\(t\)</span>, когато се намира в състояние <span class="math inline">\(s\)</span>. Решението на агента се взима благодарение на Дълбока Бейсова Невронна Мрежа (Deep Baysian Neural Network)</p>
<h3 id="определяне-на-награди"><span class="header-section-number">1.2.4</span> Определяне на награди</h3>
<p>Наградата <span class="math inline">\(r_t\)</span> за всяка стъпка <span class="math inline">\(t\)</span> се дефинира като:</p>
<p><span class="math display">\[r = -1 + C_a\]</span></p>
<p>където <span class="math inline">\(C_a\)</span> е новият процент покритие на код след избора на действие <span class="math inline">\(a\)</span>. “Наказанието”, количествено оценено с <span class="math inline">\(-1\)</span> за всяко следващо взето действие “мотивира” агента да се стреми изучи средата максимално бързо. Това спомага за намаляване на възможността за разглеждане на две съседни състояния в цикъл.</p>
<h3 id="оценка-на-модела-за-избор-на-действия"><span class="header-section-number">1.2.5</span> Оценка на модела за избор на действия</h3>
<p>Адекватността на агента, т.е. адекватността на всички избрани от агента действия се измерва чрез т.нар мярка за “съжаление” (regret) - разликата между оптималната обща награда и получената обща награда. Оптималната награда може да се постигне, когато на всяка стъпка <span class="math inline">\(t\)</span>, агентът избира оптимално действие <span class="math inline">\(a^*\)</span>.</p>
<h1 id="мотивация"><span class="header-section-number">2</span> Мотивация</h1>
<p>Една от основните цели на Изкуствения Интелект (ИИ) е да създаде агенти, които разбират и взаимодействат със света около нас. Значителен прогрес в тази насока беше постигнат през последните години благодарение на развитието на изчислителната техника (графични ускорители), наличието на голямо количество данни, нови начини за събиране и съхранението им и нови алгоритми. Бързият напредък в сферата на подсиленото обучение доведе до разработката на интелигентни агенти, взаимодействащи с все по-сложни среди <span class="citation" data-cites="mnih2015human">(Mnih et al. 2015)</span>, <span class="citation" data-cites="silver2016alphago">(Silver &amp; Hassabis 2016)</span>, <span class="citation" data-cites="levine2016end">(Levine et al. 2016)</span> и <span class="citation" data-cites="silver2017mastering">(Silver et al. 2017)</span>. Критични за това са обучаващите алгоритми, техники за скалирането им и симулационни среди, които предоставят начини за оценка и сравняване на различни агенти <span class="citation" data-cites="bellemare2013arcade">(Bellemare et al. 2013)</span>, <span class="citation" data-cites="todorov2012mujoco">(Todorov et al. 2012)</span> и <span class="citation" data-cites="johansson2016learning">(Johansson et al. 2016)</span>.</p>
<p>Хората се справят лесно с редица задачи, които изискват комплексно разбиране на визуалния свят, разпознаване на различни обекти в него и взаимодействие с тях. Например (екран от ГПИ среда и разпознаване на обекти в него). Би било лесно за човек да изучи подобна визуална среда.</p>
<p>Агенти, действащи в симулирани среди, са фундаментално ограничени - те никога не се сблъскват със сложността на реалния свят, поради което не могат да използват семантично знание и достигнат интелигентност. В роботиката агентите действат в реална среда, но процесът на обучение е бавен и скъп, дори и за тясно дефинирани задачи <span class="citation" data-cites="levine2016end">(Levine et al. 2016)</span>.</p>
<p>За справянето с тези проблеми могат да се използват среди, базирани на ГПИ приложения <span class="citation" data-cites="pmlr-v70-shi17a">(Shi et al. 2017)</span>. Те предоставят разнообразни задачи, възможност за бързо итериране и обучение. Агентите получават същите сензорни данни, които получава човек, взаимодействащ с тези среди. Те предоставят възможност за изграждане на знание, невъзможно за придобиване в симулации.</p>
<p><strong>Предизвикателства</strong> Тъй като подобни възможности изглеждат естествени за човек, може да забравим колко трудни са те за един агент. Изображенията са представени като голям масив от числа, които представят яркостта за всяка позиция. Едно такова изображение може да съдържа милиони такива <em>пиксели</em>, които агентът трябва да трансформира до семантични концепции на високо ниво, като например “текст” или “бутон”. При това, различни форми и цветове на даден бутон, също трябва да се класифицират като такъв, независимо от възможността за наличие на напълно различни шаблони (patterns) в яркостта на пикселите.</p>
<p>Концептуалното разбиране на дадено изображение е само първата стъпка за създаването на подобни агенти. Основна задача е създаване на модел на агент, който избира действия, които до доведат да постигането на поставена задача. Трудността тук се изразява в липсата на пълна информация (fully observed) за средата в която агента действа. Например, натискането на един и същи бутон в различни състояния на средата, може да доведе до наблюдаването на две напълно различни състояния на средата. Това означава, че е необходимо знание за конкретното състояние на средата.</p>
<p>Агентът няма предварителен модел на средата, която изучава. Той я “опознава”, чрез опити и грешки, като се опитва да приложи различни комбинации от действия в дадено състояние, за да постигне оптимална награда. На всяка стъпка даден агент трябва да избере дали да използва вече наученото или да избере действие, което не е изпълнил в конкретното състояние. Тази дилема се нарича: компромис на изучаване и използване на наученото (exploration exploitation tradeoff) и е основна задача за решаване от всеки агент. ГПИ средите могат да предоставят голям брой действия за дадено състояние (например меню системата на Microsoft Word), което прави пълното им изучаване неприложимо в кратки интервали от време.</p>
<p><strong>Обнадеждаващ прогрес</strong> Въпреки сложността на задачата, през последните години се наблюдава значителен прогрес в областта на подсиленото обучение. По конкретно, развитието на Изкуствените Невронни Мрежи (ИНМ) (Artificial Neural Networks) и методи за създаване на Бейсови модели с милиони параметри доведоха до значително разширение на областите в които подсиленото обучение е приложимо. Алгоритми като Дълбоко Q-обучение (Deep Q-learning) допринасят за създаване на агенти, които надхвърлят възможностите на хората в тясно дефинирани задачи <span class="citation" data-cites="mnih2015human">(Mnih et al. 2015)</span>, както и по-широко приложими такива <span class="citation" data-cites="silver2017mastering">(Silver et al. 2017)</span></p>
<p><strong>Неотговорени въпроси</strong> Основният подход, използван в много от тези приложения е създаване на модел, който работи в добре дефинирани среди или такива в които се наблюдава пълна информация. Допълнително, агентите взимат решения на базата на изчислени точкови оценки. Възможно е това да намаля ефективността на тези модели, както и обяснението на взетите решения. Открит остава въпросът дали добавянето на вероятностно разбиране за средата може да се справи с тези проблеми <span class="citation" data-cites="bellemare2017distributional">(Bellemare et al. 2017)</span> <span class="citation" data-cites="anonymous2018efficient">(Anonymous 2018)</span>.</p>
<p><strong>Принос</strong> В тази работа добавяме вероятностно разбиране за средата и разработваме модели и техники за ефективно Бейсово изучаване на ГПИ среди. Също така създаваме конкретна среда, която Агентът изучава. Например, агентът трябва да наблюдава дадено изображение и избере действие, което максимизира вероятността за постигане на поставена цел. Този модел ще опише процесът за взимане на решения използвайки вероятностни разпределения. С други думи, целта на работата е създаване на агент, който ефективно изучава визуални среди.</p>
<p><strong>Дългосрочна мотивация</strong> Основен стремеж на работата е да направи принос към изграждането и развитието на мислещи машини, както и приложи създадените модели в конкретни приложения. Техниките предложени тук са стъпка към достигането на бъдеще, в което агентите могат ефективно да взаимодействат с реалния свят (или по-сложни виртуални среди) и изпълняват комплексни задачи.</p>
<p><strong>Краткосрочна мотивация</strong> Създаване на автоматизирани тестове за оценка на качеството на софтуерен продукт използвайки агент. Търсене за семантични и логически грешки в дадена програмна ГПИ среда. Възпроизвеждане на стъпки, необходими за възпроизвеждане на грешка.</p>
<h1 id="описание-на-работата-на-агента"><span class="header-section-number">3</span> Описание на работата на агента</h1>
<h2 id="дадено-на-агента"><span class="header-section-number">3.1</span> Дадено на агента</h2>
<ol type="1">
<li>Изображение на ГПИ средата - пример ГПИ. примери + изображения</li>
<li>Изучи ГПИ средата - да достигне всички възможни състояние на средата</li>
<li>Възможни действия -</li>
</ol>
<p>Агентът щракване върху екрана (пример визуален) Агентът натиска и задържа (пример визуален) Агентът натиска и влачи курсора (пример визуален) Агентът scroll-ва нагоре и надолу</p>
<h2 id="задачи"><span class="header-section-number">3.2</span> Задачи</h2>
<ul>
<li>Да постигне 100% покритие на програмния код на даден софтуерен продукт (СП)</li>
<li>Да генерира поредица от действия с които преминава през всички клонове на дървото, описващо възможните състояния на СП</li>
</ul>
<!-- 
This is the Latex-heavy title page. 
People outside UCL may want to remove the header logo 
and add the centred logo
-->

<!-- This page is for an official declaration. -->

<p>   </p>
<h1 id="абстракт" class="unnumbered">Абстракт</h1>

<h1 id="благодарности" class="unnumbered">Благодарности</h1>
<!-- Use the \newpage command to force a new page -->




<!--
# List of figures {.unnumbered}

For me, this was the only drawback of writing in Markdown: it is not possible to add a short caption to figures and tables. This means that the \listoftables and \listoffigures commands will generate lists using the full titles, which is probably isn't what you want. For now, the solution is to create the lists manually, when everything else is finished.
-->



<h1 id="list-of-tables" class="unnumbered">List of tables</h1>
<!-- 
For me, this was the only drawback of writing in Markdown: it is not possible to add a short caption to figures and tables. This means that the \listoftables and \listoffigures commands will generate lists using the full titles, which is probably isn't what you want. For now, the solution is to create the lists manually, when everything else is finished.
-->
<p>Table 5.1 This is an example table . . . <br />
Table x.x Short title of the figure . . . </p>
<h1 id="речник" class="unnumbered">Речник</h1>


<h1 id="увод"><span class="header-section-number">4</span> Увод</h1>
<p>Съществуващите методи не дават възможност за споделяне на наученото от друго приложение, намиране на аномалии във функционалността, бързо научаване на промени (премахване на старо знание) и оценка на несигурността при изпълнение на действие.</p>
<h2 id="структура-на-дисертационния-труд"><span class="header-section-number">4.1</span> Структура на дисертационния труд</h2>
<p><strong>Глава 2</strong> дава познания върху Дълбокото подсилено обучение и Бейсовото моделиране. <strong>Глава 3</strong> поставя целите и задачите на текущата работа. В <strong>Глава 4</strong> се създава среда за тестване на Android мобилни приложения. <strong>Глава 5</strong> представя модел за генериране на входни данни за тестови случаи. В <strong>Глава 6</strong> се представя модел за намиране на аномалии в тестови случай по подадено изображение. Цялостната система е представена в <strong>Глава 7</strong> заедно с емпирични сравнения спрямо други решения. Нерешени проблеми, бъдещи подобрения и дискусия се намират в последната глава.</p>
<h1 id="литературен-обзор"><span class="header-section-number">5</span> Литературен обзор</h1>
<h2 id="подсилено-обучение"><span class="header-section-number">5.1</span> Подсилено обучение</h2>
<p>Една от основните задачи в сферата на изкуствения интелект е взимане на поредица от решения в стохастична среда. Един конкретен пример за взимане на решения в стохастична среда е агент, който изучава ГПИ. Тази задача се състои в избор на редица от решения, които да максимизират броят на разгледаните състояния на текущото приложение. Това е по-сложно от задачи, в които трябва да се направи само едно решение. Оценката за представянето на агента може да се даде само след много извършени от него стъпки. Това означава, че той може да избере неправилно действие сега и да разбере за това много по-късно, т.е. имаме <em>забавяне на последствията</em>. Допълнително, не може да наблюдаваме точното състояние на средата, поради липсата на точен модел на средата, която се изучава.</p>
<p>Основният начин за моделиране на такива среди са Марковски вериги.</p>
<p><strong>Марковските вериги за вземане на решения (MDP)</strong> моделират системи, които искаме да контролираме. Във всяка времева стъпка <span class="math inline">\(t\)</span>, системата се намира в дадено състояние <span class="math inline">\(s\)</span>. Например, описаният агент може да се намира на даден екран от приложението, след като е натиснал определен бутон. Системата преминава през различни състояния като резултат от действията, които сме избрали. Задачата ни е да избираме действия, които са добри и да минимизираме броя на тези, които не са. Разнообразни проблеми са моделирани чрез Марковски вериги (MDP формализма). Някои примери за използване на марковски вериги са системи за препоръки <span class="citation" data-cites="joachims1997webwatcher">(Joachims et al. 1997)</span>, рутиране на мрежи <span class="citation" data-cites="boyan1994packet">(Boyan et al. 1994)</span>, управление на асансьори <span class="citation" data-cites="crites1996improving">(Crites &amp; Barto 1996)</span>, навигация на роботи <span class="citation" data-cites="sutton1998reinforcement">(Sutton &amp; Barto 1998)</span>.</p>
<p>Подсиленото обучение (RL) <span class="citation" data-cites="sutton1998reinforcement">(Sutton &amp; Barto 1998)</span> дава начини за решаване на задачи, дефинирани чрез MDP формализма. Самообучаващ се агент с подсилено обучение (RL) взаимодейства със средата за определено време. На всяка времева стъпка <span class="math inline">\(t\)</span>, агентът получава състояние <span class="math inline">\(s_t\)</span> от пространството на състоянията <span class="math inline">\(S\)</span> и избира действие <span class="math inline">\(a_t\)</span> от пространство с действия <span class="math inline">\(A\)</span>, следвайки политика <span class="math inline">\(\pi(a_t|s_t)\)</span>. Политиката <span class="math inline">\(\pi\)</span> определя поведението на агента, т.е. в определено състояние <span class="math inline">\(s_t\)</span>, какво действие агентът трябва да избере. Тя дава функция за преобразуване на състояние <span class="math inline">\(s_t\)</span> до състояние <span class="math inline">\(s_{t + 1}\)</span> чрез действие <span class="math inline">\(a_t\)</span>. Използвайки дадена политика, агентът получава скаларна награда <span class="math inline">\(r_t\)</span> и преминава в следващо състояние <span class="math inline">\(s_{t + 1}\)</span>, което се определя от функцията за награди <span class="math inline">\(R(s, a)\)</span> и функцията, даваща вероятности за преминаване в друго състояние <span class="math inline">\(P(s_{t+1}|s_t,a_t)\)</span>. Когато моделът, който моделира поведението на агента е дискретен, т.е. може да се разглежда като отделни епизоди, описаният процес продължава докато агентът не достигне до крайно състояние. Тогава агентът се рестартира за започване на ново обучение. Общата награда е дефинирана като:</p>
<p><span class="math display">\[R_t = \sum_{k=0}^{\infty}\gamma^kr_{t+k}\]</span></p>
<p>представлява обезценена стойност с фактор <span class="math inline">\(\gamma \in (0,1]\)</span>. Агентът се опитва да максимизира очакваната награда във всяко състояние.</p>
<p>Функция на стойностите <span class="math inline">\(Q^\pi(s, a)\)</span> дава предсказана обща бъдеща награда, която измерва до колко са добри дадено състояние или двойка състояние-действие. Стойността на дадено действие <span class="math inline">\(Q^\pi(s, a) = E[R_t|s_t = s, a_t = a]\)</span> ни дава очакваната награда за избиране на дейсвие <span class="math inline">\(a\)</span> в състояние <span class="math inline">\(s\)</span> и следвайки фиксирана политика <span class="math inline">\(\pi\)</span>. Оптимална стойностна функция <span class="math inline">\(Q^*(s,a)\)</span> предоставя действие <span class="math inline">\(a\)</span>, което максимизира стойността на наградата за дадено състояние <span class="math inline">\(s\)</span>. Може да дефинираме функция даваща стойност на състоянията <span class="math inline">\(V^\pi(s)\)</span>, както и оптималната й версия <span class="math inline">\(V^*(s)\)</span> по сходен начин.</p>
<p>Извод: ние подсилваме обучението с въвеждане на награда.</p>
<h3 id="дълбоко-обучение"><span class="header-section-number">5.1.1</span> Дълбоко обучение</h3>
<p>Нека разгледаме един от най-простите статистически модели - линейната регресия <span class="citation" data-cites="gauss1809theoria legendre1805nouvelles">(Gauss 1809; Legendre 1805)</span>. Нека е дадено множество от <span class="math inline">\(N\)</span> входно-изходни двойки <span class="math inline">\(\{(x_1, y_1), ..., (x_n, y_n)\}\)</span>. Например, нека <span class="math inline">\(x\)</span> да е тегло в кг, а <span class="math inline">\(y\)</span> - височина в см на <span class="math inline">\(N\)</span> човека. Линейната регресия прави предположението, че съществува линейна функция, която преобразува всяко <span class="math inline">\(x_i \in \mathbb{R}^Q\)</span> към <span class="math inline">\(y_i \in \mathbb{R}^D\)</span>. Тогава нашият модел е линейна трансформация на входните данни:</p>
<p><span class="math display">\[f(x) = xW + b\]</span></p>
<p>където <span class="math inline">\(W\)</span> е <span class="math inline">\(Q \times D\)</span> матрица и <span class="math inline">\(b\)</span> е вектор от <span class="math inline">\(D\)</span> елемента. Тогава, задачата се свежда до намиране на такива параметри <span class="math inline">\(W\)</span> и <span class="math inline">\(b\)</span>, които минимизират средната квадратична грешка:</p>
<p><span class="math display">\[e = \frac{1}{N}\sum_i||y_i - (x_iW + b)||^2\]</span></p>
<p>В общия случай, връзката между <span class="math inline">\(x\)</span> и <span class="math inline">\(y\)</span> може да не е линейна. Тогава искаме да дефинираме нелинейна функция <span class="math inline">\(f(x)\)</span>, която преобразува входните данни до изходни. За тази цел може да приложим linear basis function regression (превод?) <span class="citation" data-cites="bishop2007pattern gergonne1815application">(Bishop 2007; Gergonne 1815)</span>, където входните данни <span class="math inline">\(x\)</span> се подават на <span class="math inline">\(K\)</span> фиксирани скаларни нелинейни трансформации <span class="math inline">\(\phi_k(x)\)</span> за създаване на свойствен вектор <span class="math inline">\(\Phi(x) = [\phi_1(x), ...,\phi_k(x)]\)</span>. Трансформациите <span class="math inline">\(\phi_k\)</span> наричаме базисни функции. Върху така създадения вектор се прилага линейна регресия. LBFR може да се сведе до линейна регресия, когато <span class="math inline">\(\phi_k(x) := x_k\)</span> и <span class="math inline">\(K = Q\)</span>. Този тип функции се смятат за фиксирани и взаимно ортогонални. Когато тези ограничения се пропуснат говорим за <em>параметризирани</em> базисни функции.</p>
<h4 id="изкуствени-невронни-мрежи"><span class="header-section-number">5.1.1.1</span> Изкуствени невронни мрежи</h4>
<p>Когато подредим параметризирани базисни функции в йерархия, може да говорим за изкуствени невронни мрежи. Всеки свойствен вектор в тази йерархия ще наричаме слой. Композицията от подобни слоеве води до голямата гъвкавост на тези модели. Често те постигат високи резултати на различни задачи и могат да се приложат върху реални проблеми, работещи върху терабайти от данни.</p>
<p><strong>Feed-forward neural networks.</strong> Нека разгледаме модел с един <em>скрит слой</em> <span class="citation" data-cites="rumelhart1985learning">(Rumelhart et al. 1985)</span>. Нека <span class="math inline">\(x\)</span> е вектор с <span class="math inline">\(Q\)</span> елемента, представящ входните данни. Трансформираме го с афинна трансформация до вектор с <span class="math inline">\(K\)</span> елемента. Отбелязваме с <span class="math inline">\(W_1\)</span> линейната преобразуваща матрица (матрица на теглата) и с <span class="math inline">\(b\)</span> транслацията използвана за трансформиране на <span class="math inline">\(x\)</span> за да получим <span class="math inline">\(xW_1 + b\)</span>. Върху всеки елемент на получената матрица се прилага нелинейна функция <span class="math inline">\(\sigma(\cdot)\)</span>. Резултатът е т. нар. <em>скрит слой</em>, а всеки елемент се нарича <em>мрежова единица</em>. Върху резултата се прилага втора линейна трансформация с матрица на теглата <span class="math inline">\(W_2\)</span>, която преобразува скрития слой до изходен вектор с <span class="math inline">\(D\)</span> елемента. Имаме <span class="math inline">\(Q \times K\)</span> матрица <span class="math inline">\(W_1\)</span>, <span class="math inline">\(K \times D\)</span> матрица <span class="math inline">\(W_2\)</span> и <span class="math inline">\(b\)</span> - вектор от <span class="math inline">\(K\)</span> елемента. Резултат от дадена невронна мрежа би бил:</p>
<p><span class="math display">\[\hat{y} = \sigma(xW_1 + b)W_2\]</span></p>
<p>при дадени входни данни <span class="math inline">\(x\)</span>.</p>
<p>Когато използваме невронната мрежа за решаване на регресионна задача, може да минимизираме Евклидовата грешка:</p>
<p><span class="math display">\[ e^{W_1, W_2, b}(X, Y) = \frac{1}{2N}\sum_{i=1}^{N}||y_i - \hat{y_i}||^2\]</span></p>
<p>където <span class="math inline">\(\{y_1,\dots,y_n\}\)</span> са <span class="math inline">\(N\)</span> наблюдавани изходни стойности, <span class="math inline">\(\{\hat{y_1},\dots,\hat{y_n}\}\)</span> са изходни данни от модела, а <span class="math inline">\(\{x_1,\dots,x_n\}\)</span> са входните данни. Предполагаме, че минимизирайки тази грешка спрямо <span class="math inline">\(W_1, W_2, b\)</span> ще получим модел, който генерализира добре при нови данни <span class="math inline">\(X_{\text{test}}, Y_{\text{test}}\)</span>.</p>
<p>Когато задачата е да се предскаже класът, към който <span class="math inline">\(x\)</span> принадлежи, от множеството <span class="math inline">\(\{1,\dots,D\}\)</span>, използваме същия модел. Промяната се състой в това, че прилагаме softmax функция върху получения резултат. Тази функция ни дава нормализирани оценки за всеки клас:</p>
<p><span class="math display">\[\hat{p_i} = \frac{exp(\hat{y_i})}{\sum d&#39; exp(\hat{y_i&#39;})}\]</span></p>
<p>Когато вземем логаритъма от горната функция, получаваме softmax грешка:</p>
<p><span class="math display">\[ e^{W_1, W_2, b}(X, Y) = -\frac{1}{N}\sum_{i=1}^{N}log(\hat{p}_{i, c_i})\]</span></p>
<p>където <span class="math inline">\(c_i \in \{1, \dots, D\}\)</span> е наблюдаваният клас за вход <span class="math inline">\(i\)</span>.</p>
<p>Описаният по-горе модел има проста структура, но може да бъде разширен за по-специализирани задачи. Този тип по-сложни модели се използват, когато задачите изискват обработка на поредици или изображения.</p>
<p><strong>Convolutional Neural Networks</strong> CNN е архитектура <span class="citation" data-cites="lecun1989backpropagation">(LeCun et al. 1989)</span>, която се използва при изображения. Задачи, които до скоро се смятаха за нерешими, имат решения посредством този тип модели <span class="citation" data-cites="hinton2012improving">(Hinton et al. 2012)</span>. Моделът е създаден чрез рекурсивно приложение на конволуции и обединяващи слоеве. Конволуционният слой е линейна трансформация, която запазва пространствена информация от входното изображение.</p>
<p><strong>Recurrent neural networks (RNN)</strong> RNN е модел <span class="citation" data-cites="rumelhart1985learning werbos1988generalization">(Rumelhart et al. 1985; Werbos 1988)</span>, базиран на поредици от данни, който се използва за обработка на текст, обработка на видео и други <span class="citation" data-cites="kalchbrenner2013recurrent sundermeyer2012lstm">(Kalchbrenner &amp; Blunsom 2013; Sundermeyer et al. 2012)</span>. Входните данни за RNN са поредица от символи. За всяка времева стъпка <span class="math inline">\(t\)</span>, проста невронна мрежа е приложена върху единствен символ, както и изходните данни от мрежата от предишната стъпка.</p>
<p>Конкретно, при дадена редица от входни данни <span class="math inline">\(x = [x_1,\dots,x_t]\)</span> с дължина <span class="math inline">\(T\)</span>, прост RNN модел е създаден чрез повтарящо се приложение на функция <span class="math inline">\(f_h\)</span>. Така се генерира скрито състояние <span class="math inline">\(h_t\)</span> за времева стъпка <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[h_t = f_h(x_t,h_{t-1}) = \sigma(x_tW_h + h_{t-1}U_h + b_h)\]</span></p>
<p>за някаква нелинейна функция <span class="math inline">\(\sigma\)</span>. Изходните данни от модела може да бъдат дефенирани като:</p>
<p><span class="math display">\[\hat{y} = f_y(h_T) = h_TW_y + b_y\]</span></p>
<p>Съществуват и по-сложни RNN модели, като LSTM <span class="citation" data-cites="hochreiter1997long">(Hochreiter &amp; Schmidhuber 1997)</span> и GRU <span class="citation" data-cites="cho2014learning">(Cho et al. 2014)</span>.</p>
<h3 id="дълбоко-подсилено-обучение"><span class="header-section-number">5.1.2</span> Дълбоко подсилено обучение</h3>
<p>Този тип методи се класифицират, когато използваме дълбоки невронни мрежи за апроксимиране на някой от компонентите на подсиленото обучение: функция на стойностите <span class="math inline">\(V(s;\theta)\)</span>, политика <span class="math inline">\(\pi(a|s;\theta)\)</span> или модела за промяна на състояние и награди. Параметрите <span class="math inline">\(\theta\)</span> представляват тегла в дълбоки невронни мрежи. Когато използваме “плитки” модели, като например линейна регресия, дървета за вземане на решения и др. като апроксиматори на функция, имаме “плитко” подсилено обучение с параметри <span class="math inline">\(\theta\)</span> за съответния модел. Основната разлика между дълбокото и плиткото подсилено обучение се състой в апроксиматора на функцията, която използват. Когато се използва извън политикова апроксимация - например на нелинейни функции, може да се наблюдават нестабилност и разходимост <span class="citation" data-cites="tsitsiklis1997analysis">(Tsitsiklis et al. 1997)</span>. Въпреки това, скорошната работа върху дълбоки <span class="math inline">\(Q\)</span>-мрежи <span class="citation" data-cites="mnih2015human">(Mnih et al. 2015)</span> и <em>AlphaGo</em> <span class="citation" data-cites="silver2016alphago">(Silver &amp; Hassabis 2016)</span> стабилизират процеса на обучение и постигат много добри резултати.</p>
<p>Дълбокото подсилено обучение започна рязкото си развитие с работата на <span class="citation" data-cites="mnih2015human">(Mnih et al. 2015)</span>. Преди това, RL даваше нестабилни резултати, когато се използваха нелинейни апроксиматори като невронни мрежи. Дълбоките <span class="math inline">\(Q\)</span> мрежи (DQN) направиха няколко важни приноса: 1) стабилизиране на обучението, използвайки дълбоки невронни мрежи <span class="citation" data-cites="lin1992self">(Lin 1992)</span> 2) подход за цялостно обучение без почти никакво познание за областта 3) обучаване на гъвкава невронна мрежа с еднакъв алгоритъм за изпълняване на различни задачи, например 49 Atari игри <span class="citation" data-cites="bellemare2013arcade">(Bellemare et al. 2013)</span>, на които се представят по-добре от всеки известен алгоритъм до момента.</p>
<h4 id="double-dqn"><span class="header-section-number">5.1.2.1</span> Double DQN</h4>
<p><span class="citation" data-cites="van2016deep">(Van Hasselt et al. 2016)</span> предложиха Double DQN (D-DQN) за справяне с проблема на прекалена увереност (overestimate?) на Q-learning алгоритъма. В базовият алгоритъм (както и в DQN), параметрите се обновяват според:</p>
<p><span class="math display">\[\theta_{t + 1} = \theta_t + \alpha(y_t^{\theta} - Q(s_t, a_t; \theta_t))\Delta_{\theta_t}Q(s_t,a_t;\theta_{t})\]</span></p>
<p>където</p>
<p><span class="math display">\[y_t^Q = r_{t + 1} + \gamma\max\limits_{\alpha}Q(s_{t+1},a;\theta_t)\]</span></p>
<p>така че оператора <span class="math inline">\(\max\)</span> използва еднакви стойности, за да избере и оцени дадено действие. Като следствие от това, е по-вероятно да избере недостатъчно добри стойности. Double DQN предлага да оцени алчната политика спрямо невронна мрежа, но използва друга, за да оцени стойността й. Това може да се постигне с малка промяна на DQN алгоритъма, заменяме <span class="math inline">\(y_t^Q\)</span> с:</p>
<p><span class="math display">\[y_t^{D - DQN} = r_{t +1} + \gamma Q(s_{t+1},\max\limits_{\alpha}Q(s_{t+1},a_t;\theta_t);\theta_{\bar{t}})\]</span></p>
<p>където <span class="math inline">\(\theta_t\)</span> е параметър за първата невронна мрежа, а <span class="math inline">\(\theta_{\bar{t}}\)</span> е параметър за целевата мрежа.</p>
<h4 id="асинхронни-методи"><span class="header-section-number">5.1.2.2</span> Асинхронни методи</h4>
<p><span class="citation" data-cites="mnih2016asynchronous">(Mnih et al. 2016)</span> предложи асинхронни методи за четири RL алгоритъма: Q-learning, SARSA, <span class="math inline">\(n\)</span>-step Q-learning and advantage actor-critic и asynchronous advantage actor-critic (A3C). Този подход използва паралелни агенти, които използват различни политики за изучаване на средата. Асинхронните методи могат да се изпълняват върху многоядрени процесори. Те се изпълняват много по-бързо и предоставят по-бързо обучение от други известни методи.</p>
<h2 id="бейсова-статистика"><span class="header-section-number">5.2</span> Бейсова статистика</h2>
<p>Избиране на следващо действие по време на създаване на тестов случай пряко зависи от увереността във взимането на правилното решение. Несигурността от избиране на действие може да бъде моделирана посредством Бейсов подход.</p>
<p>Нека <span class="math inline">\(\theta\)</span> е неизвестна стойност, която може да е скаларна, векторна или матрица. Методите за статистически извод (inference) могат да ни помогнат да я намерим. Класическият статистически подход третира <span class="math inline">\(\theta\)</span> като фиксирана стойност. Единствената информация, която използваме за намиране на неизвестната стойност, идва от данните, с които разполагаме. Изводът се базира на резултат, получен от фунцкията на правдоподобие на <span class="math inline">\(\theta\)</span>, която свързва стойности от <span class="math inline">\(p(y|\theta)\)</span> с всяка възможност на <span class="math inline">\(\theta\)</span>, където <span class="math inline">\(y = (y_1,...,y_n)\)</span> е вектор с наблюдавани стойности.</p>
<p>Бейсовият подход третира <span class="math inline">\(\theta\)</span> като случайна стойност. За достигане на извод се използва разпределението на параметри при дадени данни <span class="math inline">\(p(\theta|y)\)</span>. Това разпределение се нарича апостериорно. Освен функцията на правдоподобие, Бейсовият подход включва априорно разпределение <span class="math inline">\(p(\theta)\)</span>, което представя вярванията ни за <span class="math inline">\(\theta\)</span> преди да се разгледат данните.</p>
<p>Теоремата на Бейс дава връзка между фунцкията на правдоподобие и априорното разпределение:</p>
<p><span class="math display">\[p(\theta|y) = \frac{p(\theta|y)p(\theta)}{p(y)}\]</span></p>
<p>където:</p>
<p><span class="math display">\[p(y) = \int p(y|\theta)p(\theta)d\theta\]</span></p>
<p>Формулата на Бейс може да бъде пренаписана по следния начин:</p>
<ol class="example" type="1">
<li><span class="math display">\[p(\theta|y) \propto p(\theta|y)p(\theta)\]</span></li>
</ol>
<p>тъй като <span class="math inline">\(p(y)\)</span> не зависи от <span class="math inline">\(\theta\)</span></p>
<p>Когато <span class="math inline">\(\theta\)</span> е многомерна величина може да напишем уравнение (1) използвайки маргиналните апостериорни разпределения като например:</p>
<p><span class="math display">\[p(\theta_1|y) = \int p(\theta|y)d\theta_2\]</span></p>
<p>където <span class="math inline">\(\theta = (\theta_1, \theta_2)\)</span>. В много случаи резултатите са многомерни и точни изводи може да бъдат направени само аналитично. Поради тази причина често се използват приближения.</p>
<h3 id="монте-карло-алгоритми-за-марковски-вериги-mcmc"><span class="header-section-number">5.2.1</span> Монте Карло алгоритми за Марковски Вериги (MCMC)</h3>
<p>MCMC алгоритмите правят неявно интегриране като взимат извадки от апостериорното разпределение. По този начин се намират приближения на стойностите, от които се интересуваме.</p>
<p>В съществото си тези методи създават Марковска верига с апостериорното разпределение на параметрите като стационарно разпределение. Когато веригата е крайна и повтаряща се, стойността на <span class="math inline">\(\theta\)</span> може да бъде оценена от извадки на средни пътища. Генерираните извадки <span class="math inline">\(\theta^{(t)}, t=1, \ldots, N\)</span> от това разпределение дават представа за целевото разпределение.</p>
<h4 id="метрополис-хастингс-алгоритъм"><span class="header-section-number">5.2.1.1</span> Метрополис-Хастингс алгоритъм</h4>
<p>Този алгоритъм е предложен от Metropolis <span class="citation" data-cites="metropolis1953equation">(Metropolis et al. 1953)</span> и по-късно генерализиран от Hastings <span class="citation" data-cites="hastings1970monte">(Hastings 1970)</span>. Методът създава Марковска верига с желаното стационарно разпределение. Алгоритъмът избира кандидат стойност <span class="math inline">\(\theta&#39;\)</span> от предварително избрано разпределение <span class="math inline">\(q(\theta, \theta&#39;)\)</span>, където <span class="math inline">\(\theta&#39; \neq \theta\)</span>. Избраната стойност <span class="math inline">\(\theta&#39;\)</span> се проверява чрез приеми-откажи метод (accept-reject step), за да се подсигури, че принадлежи на целевото разпределение.</p>
<h4 id="извадки-на-гибс"><span class="header-section-number">5.2.1.2</span> Извадки на Гибс</h4>
<p>Този метод, предложен от Geman и Geman <span class="citation" data-cites="geman1984stochastic">(Geman &amp; Geman 1984)</span>, често се представя като специален случай на Метрополис-Хастингс алгоритъма.</p>
<h3 id="извод-със-свободни-вариационни-параметри"><span class="header-section-number">5.2.2</span> Извод със свободни вариационни параметри</h3>
<p>Variational Inference (VI) методите обикновено предлагат по-добри резултати спрямо MCMC, когато времето за изпълнение е ограничено. Допълнително предимство на тези подходи е, че те са детерминирани. Систематичната грешка и дисперсията се приближават до 0 при MCMC методите, за колкото повече време бъдат оставени да се изпълняват те. Тези свойства правят MCMC алгоритмите много ефективни на теория. В практиката обаче, времето за изпълнение и изчислителната мощ са ограничени. Това налага търсенето на по-бързо методи дори когато това намаля точността на получените резултати.</p>
<p>Този тип методи дефинират приближено вариационно разпределение <span class="math inline">\(q_\omega(\theta)\)</span>, параметризирано от <span class="math inline">\(\omega\)</span>, с лесна за оценяване структура. Искаме приближеното разпределение да е максимално близко до това на апостериорното. За целта свеждаме задачата до оптимизационна и минимизираме Kullback-Leibler (KL) <span class="citation" data-cites="kullback1951information">(Kullback &amp; Leibler 1951)</span> отклонението спрямо <span class="math inline">\(\omega\)</span>. Интуитивно, KL измерва приликата между две разпределения:</p>
<p><span class="math display">\[KL(q\omega(\theta)\,||\,p(\theta|x, y)) = \int q\omega(\theta)log\frac{q\omega(\theta)}{p(\omega|x, y)}d\omega\]</span></p>
<p>(Define x,y - dataset)</p>
<p>Този интеграл е дефиниран, когато <span class="math inline">\(q\omega(\theta)\)</span> е непрекъсната спрямо <span class="math inline">\(p(\theta|x, y)\)</span>. Нека <span class="math inline">\(q^*_\omega(\theta)\)</span> е минимизираща точка (може да е локален минимум). Тогава KL може да ни даде приближение на апостериорното разпределение:</p>
<p><span class="math display">\[p(y^*|x^*, x, y) \approx \int p(y^*|x^*, \theta)q^*_\omega(\theta)d\theta =: q^*_\omega(y^*|x^*)\]</span></p>
<p>VI методите заменят изчисляването на интеграли с такова на производни. Това е много подобно на оптимизационните методи използвани в DL. Основната разлика се състой в това, че оптимизацията е върху разпределения, а не точкови оценки. Този подход запазва много от предимствата на Бейсовото моделиране и представя вероятностни модели, които дават оценка на несигурността в изводите си.</p>
<h3 id="бейсови-невронни-мрежи"><span class="header-section-number">5.2.3</span> Бейсови Невронни Мрежи</h3>
<p>Един от големите недостатъци на съществуващите архитектури на невронни мрежи е, че изводите, които получаваме, са оценки на точки. Моделите не казват до колко са сигурни в предложените резултати. Когато например един лекар получи резултат от даден модел, той трябва да знае защо и как моделът е стигнал до него. Бейсовата статистика може да даде отговор на тези въпроси <span class="citation" data-cites="gal2015dropout">(Gal &amp; Ghahramani 2015)</span>. Дори при модели използващи RNN, Бейсова интерпретация на задачата дава по-добри резултати от съществуващи такива <span class="citation" data-cites="gal2016theoretically">(Gal &amp; Ghahramani 2016)</span>.</p>
<p>Бейсови невронни мрежи, предложени в края на 80-те години <span class="citation" data-cites="kononenko1989bayesian">(Kononenko 1989)</span> и задълбочено изучавани по-късно <span class="citation" data-cites="mackay1992practical neal2012bayesian">(MacKay 1992; Neal 2012)</span>, предлагат вероятностна интерпретация на моделите за дълбоко обучение, като представят теглата им като вероятностни разпределения. Този тип модели са устойчиви на пренастройване (overfitting), предлагат оценки на несигурността и могат да се тренират върху малко на брой данни.</p>
<h2 id="автоматизирано-тестване-на-гпи"><span class="header-section-number">5.3</span> Автоматизирано тестване на ГПИ</h2>
<p>Проверката за правилно поведение на софтуер продукт е неизменна част от създаването му. Откриване и поправяне на всички потенциални проблеми преди той да бъде доставен до крайния потребител може да се сметне за най-добър случай.</p>
<h3 id="автоматизирано-тестване-на-android-приложения"><span class="header-section-number">5.3.1</span> Автоматизирано тестване на Android приложения</h3>
<p>Мобилните приложения също имат нужда от проверка на качеството. Поради тази причина, в последните години засилено се разглеждат начини за автоматизацията на подобен вид тестове. Много голяма част от извършената работа до момента се състои в създаване на входни данни за приложения за мобилната операционна система Android. Подходите използвани до момента, се различават по начина, по който създават входнни данни и изучават и използват евристики за приложението.</p>
<h4 id="съществуващи-системи"><span class="header-section-number">5.3.1.1</span> Съществуващи системи</h4>
<p><strong>Dynodroid</strong> <span class="citation" data-cites="machiry2013dynodroid">(Machiry et al. 2013)</span> е инструмент, който се базира на случайно изучаване. Предлага се и ръчен начин за въвеждане на входнни данни, когато системата е заседнала.</p>
<p><strong>MobiGUITAR</strong> <span class="citation" data-cites="amalfitano2015mobiguitar">(Amalfitano et al. 2015)</span> строи модел на приложението по време на тестване. За всяко ново състояние се поддържа списък с възможни действия, които се изпълняват използвайки DFS (depth first search) стратегия.</p>
<p><strong>SwiftHand</strong> <span class="citation" data-cites="choi2013guided">(Choi et al. 2013)</span> се опитва да максимизира покритието на код за тестваното приложение. Допълнително, инструментът се старае да минимизира броя рестартирания на приложението. SwiftHand генерира единствено докосвания и скролвания.</p>
<p><strong>PUMA</strong> <span class="citation" data-cites="hao2014puma">(Hao et al. 2014)</span> предлага генерална среда за автоматизиране на ГПИ. Инструментът предлага рамка за програмиране, в която могат да бъдат имплементирани различни стратегии за изучаване на тестваното приложение.</p>
<h4 id="покритие-на-код"><span class="header-section-number">5.3.1.2</span> Покритие на код</h4>
<p>Една от основните цели на системите за автоматизирано тестване на софтуер е да постигнат максимално покритие на програмния код. Няколко решения се опитват да постигнат това и за операционната система на Android.</p>
<p><strong>BBoxTester</strong> <span class="citation" data-cites="zhauniarovich2015towards">(Zhauniarovich et al. 2015)</span> е рамка за изготвяне на доклади относно покритието на програмния код, без той да бъде наличен. За разлика от други подобни системи, BBoxTester предлага детайлни метрики за покритието на отделни класове, методи и т.н. В основата на системата се използва друг софтуерен продукт - Emma <span class="citation" data-cites="roubtsov2005emma">(Roubtsov &amp; others 2005)</span>. BBoxTester е система с отворен код, намираща се на <a href="https://github.com/zyrikby/BBoxTester" class="uri">https://github.com/zyrikby/BBoxTester</a>. За съжаление системата е неподдържана (от 2015г.) и несъвместима с нови версии на Android.</p>
<p><strong>CovDroid</strong> <span class="citation" data-cites="yeh2015covdroid">(Yeh &amp; Huang 2015)</span> е друга система за тестване посредством подход на черната кутия (black-box testing). Програмният код на продукта не е наличен. CovDroid изчислява покритието на код като инструментира кода на приложението и използва Android Debug Bridge (adb) за да наблюдава изхода от изпълнение на програмата.</p>
<p><strong>ABCA</strong> <span class="citation" data-cites="huang2015abca">(Huang et al. 2015)</span> използва подход, много близък до този на CovDroid. Софтуерният пакет може да бъде намерен на <a href="http://cc.ee.ntu.edu.tw/~farn/tools/abca/" class="uri">http://cc.ee.ntu.edu.tw/~farn/tools/abca/</a>. По време на този обзор, страницата на инструмента не беше активна. Авторите на статията не отговориха на запитването за активен адрес за изтегляне на ABCA.</p>
<p><strong>GUITracer</strong> <span class="citation" data-cites="molnar2015live">(Molnar 2015)</span> представя иновативен подход за визуализация на покритието на код, когато приложението е базирано на ГПИ. Основен недостатък на системата е ограничението за работа върху Java AWT, SWING или SWT рамки за изграждане на ГПИ.</p>
<p><strong>GroddDroid</strong> <span class="citation" data-cites="abraham2015grodddroid">(Abraham et al. 2015)</span> предлага автоматично намиране и изпълнение на зловреден софтуер (malware). Системата предлага и измерване на покрит код. Софтуерът може да бъде намерен на <a href="http://kharon.gforge.inria.fr/" class="uri">http://kharon.gforge.inria.fr/</a>. Програмният код е ясно документиран и лесен за употреба. Един недостатък е използването на Logcat монитора за извличане на метрики за покритие на кода. Повечето от горепосочените системи използват този подход.</p>
<h4 id="текущо-състояние-state-of-the-art"><span class="header-section-number">5.3.1.3</span> Текущо състояние (State of the art?)</h4>
<p><span class="citation" data-cites="choudhary2015automated">(Choudhary et al. 2015)</span></p>
<h3 id="проверка-на-качеството"><span class="header-section-number">5.3.2</span> Проверка на качеството</h3>
<ul>
<li>Достатъчно бързо ли е? (Model should monitor for speed exec anomalies or report just slow parts)</li>
<li>Как да повторя грешката? (Provide/execute steps for reproduction)</li>
<li>Има ли разлики в изходните данни? (Change in hierarchy/image screenshot)</li>
</ul>
<h1 id="цел-и-задачи"><span class="header-section-number">6</span> Цел и задачи</h1>
<p>Целта на настоящата дисертацаионна работа е да създаде система за автоматизирано тестване на ГПИ, която използва за входни данни само визуалния изход на тестваното приложение. За постигане на целта трябва да се изпълнят следните задачи:</p>
<ul>
<li>Избор на подходящи оценъчни функции и награди, които да мотивират максималното покритие на програмен код по време на тестване</li>
<li>Създаване на структури, в които да се запазват поредиците от стъпки, необходими за повтаряне на тестови случай</li>
<li>Създаване на модел, който генерира поредица от действия, използвани за играждане на тестовите случаи</li>
<li>Създаване на модел, който намира аномалии по време на изпълнение на програмата</li>
<li>Автоматично именуване на отделни екрани и действия с цел улесняване на разбирането</li>
<li>Създаване и провеждане на експерименти, които да сравнят преложения модел с вече съществуващи такива</li>
<li>По подадено изображение, йерархия на изгледите и действия, да се определи кои действия са валидни върху кои елементи</li>
</ul>
<h1 id="среда-за-изучаване-rl-exploration-на-гпи-приложения"><span class="header-section-number">7</span> Среда за изучаване (RL exploration) на ГПИ приложения</h1>
<p>Много от съществуващите системи за автоматизирано тестване на Android приложения се опитват да изградят решения, които взимат предвид недостатъците при тестване на приложения. Някои от трудностите повече не съществуват благодарение на напредъка на модерния компютърен хардуер, а други могат да бъдат решени много по-ефективно благодарение на новъведени инструменти за разработка за Android. Например, <strong>SwiftHand</strong> <span class="citation" data-cites="choi2013guided">(Choi et al. 2013)</span> се опитва да намали нуждата от преинсталиране на приложението върху устройството. В по-новите си версии, adb, предлага способ за изчистване на състоянието на дадено приложение, без нужда от преинсталирането му.</p>
<p>От особена важност за изграждане на алгоритъм в среда за подсиленото обучение е наличието на награда. Повечето от изградените системи се опитват да максимизират покритието на код. В практиката тази метрика е важна, но и недостатъчна. Фактът, че дадена част от програмния код се е изпълнила и не е предизвикала грешка в програмата не означава, че поведението на програмата е правилно (или не се е променило без това бъде желания ефект от разработчиците).</p>
<p>За нуждите на текущата работа и всеки желаещ да използва се предлага обща среда за изучване/тестване на мобилни приложения. Поради липсата на други свободни инструменти (или такива, които са използваеми). Системата е свободна за използване, с отворен код и може да бъде намерена на <a href="https://github.com/appgym/appgym" class="uri">https://github.com/appgym/appgym</a>.</p>
<h2 id="android-специфична-среда"><span class="header-section-number">7.1</span> Android специфична среда</h2>
<p>Средата се състои от два основни компонента - клиент и сървър. Сървърът работи върху Android устройството и предоставя данни за постигнатото покритие на код, изпълнение на действията, генерирани от модела и изображение за текущото състояние. Клиентът предоставя възможните действия на модела, както и комуникира със сървъра за да представи неговата функционалност.</p>
<p>Клиентът предоставя интерфейс към средата подобен на този на OpenAI gym <span class="citation" data-cites="brockman2016openai">(Brockman et al. 2016)</span>. Двата основни метода, които реализира са <code>reset()</code> и <code>step(action)</code>. <code>reset()</code> предоставя възможност на средата да се върне до първоначално състояние. Това се постига чрез спиране на приложението (ако то е стартирано), изтриване на данните поддържащи състоянието му, стартирането му и предоставяне на образ от екрана, както и възможните действия за състоянието. Изброената функционалност се реализира посредством adb команди и библиотеката <code>uiautomator</code> <a href="https://github.com/xiaocong/uiautomator" class="uri">https://github.com/xiaocong/uiautomator</a>. Методът <code>step(action)</code> изпълнява избраното действие и предоставя новото състояние на средата, заедно с получената награда и новите възможни действия. Тук също се взима решение дали текущия епизод от обучението е приключил.</p>
<p>Множеството от възможните действия за текущото състояние се базират на броя и видовете графични елементи в него. Всеки елемент върху който може да се извърши докосване, задържане, скролиране, влачене и т.н. се превръща в действие. Множеството от графични елементи се извлича посредством библиотеката <code>uiautomator</code>.</p>
<p>Наградата за всяка избрана стъпка пряко се базира на покритието на код за текущия епизод. Стойността се изменя в интервала <span class="math inline">\([0;1.0]\)</span> и е нарастваща. Получаването на наградата след всяка стъпка е необходимо за обучение на модела. Скоростта на изпълнение пряко влияе на общото бързодействие на системата. За намиране на текущото покритие на код и изграждане на доклад се използва <code>JaCoCo</code> <span class="citation" data-cites="hoffmann2009jacoco">(Hoffmann et al. 2009)</span>. <code>JaCoCo</code> е интегриран в инструментите за разработка на Android и се използва основно, когато е нужно покритие на код базирано на преминали тестове.</p>
<p>За целта на текущата работата бяха направени някои промени, които предоставят възможност за извличане на необходимите данни, докато програмата се изпълнява и не е в тестова среда. Няколко подхода бяха изпробвани за изграждане на крайните доклади. Първоначално бяха използвани adb команди и генериране на доклад посредством gradle задача. Бързодействието не беше задоволително - необходими бяха около 2 секунди на съвременен мобилен компютър. Около повината от времето се губеше в генериране на доклад, който предоставя повече от необходимата информация.</p>
<p>Крайното решение използва комбинация от <code>HTTP</code> сървър на устройството, клиент, специализиран начин за записване на данните за покритие на код и специализиран генератор за доклади. Допълнително бързодействие се постига чрез Nailgun <a href="https://github.com/martylamb/nailgun" class="uri">https://github.com/martylamb/nailgun</a> сървър, който изпълнява генератора за доклади. Така описаните оптимизации извършват необходимата работа за около 20 милисекунди (или 0.02 секунди) на същия компютър.</p>
<p>Взимането на текущото състояние се състой в направата на изображение на текущия екран на устройството. Тази задача отново се извършва от библиотеката <code>uiautomator</code>. Изображението може да бъде намалено до желани размери в зависимост от изискванията на задачата. Така полученото изображение се представя за текущо състояние под формата на тензор.</p>
<h2 id="web-специфична-среда"><span class="header-section-number">7.2</span> Web специфична среда</h2>
<p>Web средата предоставя възможност за изучаване на мобилни web приложения. Тя използва библиотеката <code>pyppeteer</code>, която предоставя автоматизиран достъп до Web Browser.</p>
<p>Системата AppGym е структурирана така че да може да се използва с други мобилни и настолни операционни системи. Интерфейсът е много сходен до този на OpenAI gym. Възможно е рамката да бъде променена, така че да бъде използвана за други задачи и други цели.</p>
<h1 id="изучаване-на-визуални-среди-learning-to-explore-visual-environments"><span class="header-section-number">8</span> Изучаване на визуални среди (Learning to explore visual environments)</h1>
<p>В тази глава изграждаме подход, който ни позволява да изучаваме среда, чието състояние се базира на изображения. По зададено изображение и множество от действия трябва да изберем действие, което максимизира изучената част от средата. По-конкретно, текущото състояние се определя от изображението на приложението и възможните действия с графичните елементи, а изучената част е моментното покритие на код.</p>
<p>Подходът, който използваме се базира изцяло на данните, които средата предоставя. В частност, разработваме модели, които представляват дълбока невронна мрежа, която приема изображения като входни данни и избира действие. Така създадения модел се тренира върху мобилни приложения за Android. Експерименталните резултати показват…</p>
<h2 id="related-work"><span class="header-section-number">8.1</span> Related Work</h2>
<p>Извличане на характеристики (features) от сензорна информация намалява нуждата от ръчното им закодиране и увеличава скоростта на изграждане на модели. Скорошните открития в областта на дълбокото самообучение доведоха до големи открития в компютърното зрение <span class="citation" data-cites="krizhevsky2012imagenet mnih2013machine sermanet2013pedestrian">(Krizhevsky et al. 2012; Mnih 2013; Sermanet et al. 2013)</span> и гласовото разпознаване <span class="citation" data-cites="dahl2012context graves2013speech">(Dahl et al. 2012; Graves et al. 2013)</span>. Те използват различни архитектури за дълбоки невронни мрежи, включително конволуции, многослойни персептрони (multilayer perceptrons) и рекурентни невронни мрежи. Използвани са в обучение с учител, без учител. Дълбока конволюционна невронна мрежа беше използвана за създаване на агент, който играе Atari 2600 компютърни игри <span class="citation" data-cites="mnih2013playing">(Mnih et al. 2013)</span>. За входни данни е използван видео вход с размери 84x84 пиксела и възможните действия. Работата използва повторение на преживяното (experience replay) <span class="citation" data-cites="lin1992self">(Lin 1992)</span> и надгражда върху Neural Fitted Q-Learning (NFQ) <span class="citation" data-cites="riedmiller2005neural">(Riedmiller 2005)</span>. Още по-добри резултати бяха постигнати като се използва Монте Карло дървета за планиране, които бавно достигат извод, за обучение на дълбоки невронни мрежи, които са многократно по-бързи <span class="citation" data-cites="guo2014deep">(Guo et al. 2014)</span>.</p>
<h2 id="модел"><span class="header-section-number">8.2</span> Модел</h2>
<p>Нека всяко състояние <span class="math inline">\(s\)</span> е представено като изображение и имаме множество от действия <span class="math inline">\(A\)</span>, които отговарят на различни графични елементи на екрана. Искаме да изберем действие <span class="math inline">\(a\)</span>, което максимизира покритието на код. Предизвикателството се състой в намирането на възможно най-кратките поредици от действия, когато <span class="math inline">\(A\)</span> може да е различно за всяко състояние <span class="math inline">\(s\)</span>.</p>
<p>Подобно на <span class="citation" data-cites="mnih2013playing">(Mnih et al. 2013)</span> създаваме дълбока конволюционна невронна мрежа, която използва изображения за входни данни.Директна работа с реалните размери на изображение взето от устройството може да изисква прекалено много изчисления (computationally demanding). Съвременните мобилни устройства достигат до 3840x2160 разделителна способност (Syny Xperia Z5 Premium). Въпреки това, физическият размер на екраните на смартфоните достигат до 5.5“. Това ги прави далеч по-малки от екраните на настолните и преносимите компютри. Основното взаимодействие с мобилните устройства се извършва чрез различни жестове (натискане, задържане, приплъзване и т.н.) с екрана. Предвид физическите размери на пръстите на човек, екраните не могат да съдържат голямо множество от елементи с които може да се взаимодейства.</p>
<h1 id="експерименти-и-резултати"><span class="header-section-number">9</span> Експерименти и резултати</h1>
<h1 id="заключение"><span class="header-section-number">10</span> Заключение</h1>
<h2 id="нерешени-проблеми"><span class="header-section-number">10.1</span> Нерешени проблеми</h2>
<h2 id="бъдеща-работа"><span class="header-section-number">10.2</span> Бъдеща работа</h2>
<h2 id="дискусия"><span class="header-section-number">10.3</span> Дискусия</h2>
<h1 id="приложение-1-някои-важни-вероятностни-разпределения" class="unnumbered">Приложение 1: Някои важни вероятностни разпределения</h1>
<!-- 
This could be a list of papers by the author for example 
-->
<h1 id="приложение-2-фигури" class="unnumbered">Приложение 2: Фигури</h1>
<!-- 
This could include extra figures or raw data
-->

<!-- 
Do not edit this page.

References are automatically generated from the BibTex file (References.bib)

...which you should create using your reference manager.
-->
<h1 id="литература" class="unnumbered">Литература</h1>
<div id="refs" class="references">
<div id="ref-abraham2015grodddroid">
<p>Abraham, A. et al., 2015. GroddDroid: A gorilla for triggering malicious behaviors. In <em>Malicious and unwanted software (malware), 2015 10th international conference on</em>. IEEE, pp. 119–127.</p>
</div>
<div id="ref-amalfitano2015mobiguitar">
<p>Amalfitano, D. et al., 2015. MobiGUITAR: Automated model-based testing of mobile apps. <em>IEEE Software</em>, 32(5), pp.53–59.</p>
</div>
<div id="ref-anonymous2018efficient">
<p>Anonymous, 2018. Efficient exploration through bayesian deep q-networks. <em>International Conference on Learning Representations</em>. Available at: <a href="https://openreview.net/forum?id=Bk6qQGWRb" class="uri">https://openreview.net/forum?id=Bk6qQGWRb</a>.</p>
</div>
<div id="ref-bellemare2017distributional">
<p>Bellemare, M.G., Dabney, W. &amp; Munos, R., 2017. A distributional perspective on reinforcement learning. <em>arXiv preprint arXiv:1707.06887</em>.</p>
</div>
<div id="ref-bellemare2013arcade">
<p>Bellemare, M.G. et al., 2013. The arcade learning environment: An evaluation platform for general agents. <em>J. Artif. Intell. Res.(JAIR)</em>, 47, pp.253–279.</p>
</div>
<div id="ref-bishop2007pattern">
<p>Bishop, C., 2007. Pattern recognition and machine learning (information science and statistics), 1st edn. 2006. Corr. 2nd printing edn. <em>Springer, New York</em>.</p>
</div>
<div id="ref-boyan1994packet">
<p>Boyan, J.A., Littman, M.L. &amp; others, 1994. Packet routing in dynamically changing networks: A reinforcement learning approach. <em>Advances in neural information processing systems</em>, pp.671–671.</p>
</div>
<div id="ref-brockman2016openai">
<p>Brockman, G. et al., 2016. OpenAI gym. <em>arXiv preprint arXiv:1606.01540</em>.</p>
</div>
<div id="ref-cho2014learning">
<p>Cho, K. et al., 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. <em>arXiv preprint arXiv:1406.1078</em>.</p>
</div>
<div id="ref-choi2013guided">
<p>Choi, W., Necula, G. &amp; Sen, K., 2013. Guided gui testing of android apps with minimal restart and approximate learning. In <em>ACM sigplan notices</em>. ACM, pp. 623–640.</p>
</div>
<div id="ref-choudhary2015automated">
<p>Choudhary, S.R., Gorla, A. &amp; Orso, A., 2015. Automated test input generation for android: Are we there yet?(E). In <em>Automated software engineering (ase), 2015 30th ieee/acm international conference on</em>. IEEE, pp. 429–440.</p>
</div>
<div id="ref-crites1996improving">
<p>Crites, R.H. &amp; Barto, A.G., 1996. Improving elevator performance using reinforcement learning. <em>Advances in neural information processing systems</em>, 8.</p>
</div>
<div id="ref-dahl2012context">
<p>Dahl, G.E. et al., 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, 20(1), pp.30–42.</p>
</div>
<div id="ref-gal2016theoretically">
<p>Gal, Y. &amp; Ghahramani, Z., 2016. A theoretically grounded application of dropout in recurrent neural networks. In <em>Advances in neural information processing systems</em>. pp. 1019–1027.</p>
</div>
<div id="ref-gal2015dropout">
<p>Gal, Y. &amp; Ghahramani, Z., 2015. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. <em>arXiv preprint arXiv:1506.02142</em>, 2.</p>
</div>
<div id="ref-gauss1809theoria">
<p>Gauss, C.F., 1809. <em>Theoria motus corporum coelestium in sectionibus conicis solem ambientium auctore carolo friderico gauss</em>, sumtibus Frid. Perthes et IH Besser.</p>
</div>
<div id="ref-geman1984stochastic">
<p>Geman, S. &amp; Geman, D., 1984. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. <em>IEEE Transactions on pattern analysis and machine intelligence</em>, (6), pp.721–741.</p>
</div>
<div id="ref-gergonne1815application">
<p>Gergonne, J., 1815. Application de la méthode des moindres quarrésa l’interpolation des suites. <em>Annales de Math. Pures et Appl</em>, 6, pp.242–252.</p>
</div>
<div id="ref-graves2013speech">
<p>Graves, A., Mohamed, A.-r. &amp; Hinton, G., 2013. Speech recognition with deep recurrent neural networks. In <em>Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</em>. IEEE, pp. 6645–6649.</p>
</div>
<div id="ref-guo2014deep">
<p>Guo, X. et al., 2014. Deep learning for real-time atari game play using offline monte-carlo tree search planning. In <em>Advances in neural information processing systems</em>. pp. 3338–3346.</p>
</div>
<div id="ref-hao2014puma">
<p>Hao, S. et al., 2014. Puma: Programmable ui-automation for large-scale dynamic analysis of mobile apps. In <em>Proceedings of the 12th annual international conference on mobile systems, applications, and services</em>. ACM, pp. 204–217.</p>
</div>
<div id="ref-hastings1970monte">
<p>Hastings, W.K., 1970. Monte carlo sampling methods using markov chains and their applications. <em>Biometrika</em>, 57(1), pp.97–109.</p>
</div>
<div id="ref-hinton2012improving">
<p>Hinton, G.E. et al., 2012. Improving neural networks by preventing co-adaptation of feature detectors. <em>arXiv preprint arXiv:1207.0580</em>.</p>
</div>
<div id="ref-hochreiter1997long">
<p>Hochreiter, S. &amp; Schmidhuber, J., 1997. Long short-term memory. <em>Neural computation</em>, 9(8), pp.1735–1780.</p>
</div>
<div id="ref-hoffmann2009jacoco">
<p>Hoffmann, M. et al., 2009. Jacoco code coverage tool. Online, 2009.</p>
</div>
<div id="ref-huang2015abca">
<p>Huang, S.-Y. et al., 2015. ABCA: Android black-box coverage analyzer of mobile app without source code. In <em>Progress in informatics and computing (pic), 2015 ieee international conference on</em>. IEEE, pp. 399–403.</p>
</div>
<div id="ref-joachims1997webwatcher">
<p>Joachims, T. et al., 1997. Webwatcher: A tour guide for the world wide web. In <em>IJCAI (1)</em>. Citeseer, pp. 770–777.</p>
</div>
<div id="ref-johansson2016learning">
<p>Johansson, F., Shalit, U. &amp; Sontag, D., 2016. Learning representations for counterfactual inference. In <em>International conference on machine learning</em>. pp. 3020–3029.</p>
</div>
<div id="ref-kalchbrenner2013recurrent">
<p>Kalchbrenner, N. &amp; Blunsom, P., 2013. Recurrent continuous translation models. In <em>EMNLP</em>. p. 413.</p>
</div>
<div id="ref-kononenko1989bayesian">
<p>Kononenko, I., 1989. Bayesian neural networks. <em>Biological Cybernetics</em>, 61(5), pp.361–370.</p>
</div>
<div id="ref-krizhevsky2012imagenet">
<p>Krizhevsky, A., Sutskever, I. &amp; Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. In <em>Advances in neural information processing systems</em>. pp. 1097–1105.</p>
</div>
<div id="ref-kullback1951information">
<p>Kullback, S. &amp; Leibler, R.A., 1951. On information and sufficiency. <em>The annals of mathematical statistics</em>, 22(1), pp.79–86.</p>
</div>
<div id="ref-lecun1989backpropagation">
<p>LeCun, Y. et al., 1989. Backpropagation applied to handwritten zip code recognition. <em>Neural computation</em>, 1(4), pp.541–551.</p>
</div>
<div id="ref-legendre1805nouvelles">
<p>Legendre, A.M., 1805. <em>Nouvelles méthodes pour la détermination des orbites des comètes</em>, F. Didot.</p>
</div>
<div id="ref-levine2016end">
<p>Levine, S. et al., 2016. End-to-end training of deep visuomotor policies. <em>Journal of Machine Learning Research</em>, 17(39), pp.1–40.</p>
</div>
<div id="ref-lin1992self">
<p>Lin, L.-J., 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. <em>Machine learning</em>, 8(3-4), pp.293–321.</p>
</div>
<div id="ref-machiry2013dynodroid">
<p>Machiry, A., Tahiliani, R. &amp; Naik, M., 2013. Dynodroid: An input generation system for android apps. In <em>Proceedings of the 2013 9th joint meeting on foundations of software engineering</em>. ACM, pp. 224–234.</p>
</div>
<div id="ref-mackay1992practical">
<p>MacKay, D.J., 1992. A practical bayesian framework for backpropagation networks. <em>Neural computation</em>, 4(3), pp.448–472.</p>
</div>
<div id="ref-metropolis1953equation">
<p>Metropolis, N. et al., 1953. Equation of state calculations by fast computing machines. <em>The journal of chemical physics</em>, 21(6), pp.1087–1092.</p>
</div>
<div id="ref-mnih2013machine">
<p>Mnih, V., 2013. <em>Machine learning for aerial image labeling</em>. PhD thesis. University of Toronto.</p>
</div>
<div id="ref-mnih2016asynchronous">
<p>Mnih, V. et al., 2016. Asynchronous methods for deep reinforcement learning. In <em>International conference on machine learning</em>.</p>
</div>
<div id="ref-mnih2013playing">
<p>Mnih, V. et al., 2013. Playing atari with deep reinforcement learning. <em>arXiv preprint arXiv:1312.5602</em>.</p>
</div>
<div id="ref-mnih2015human">
<p>Mnih, V. et al., 2015. Human-level control through deep reinforcement learning. <em>Nature</em>, 518(7540), pp.529–533.</p>
</div>
<div id="ref-molnar2015live">
<p>Molnar, A.-J., 2015. Live visualization of gui application code coverage with guitracer. In <em>Software visualization (vissoft), 2015 ieee 3rd working conference on</em>. IEEE, pp. 185–189.</p>
</div>
<div id="ref-neal2012bayesian">
<p>Neal, R.M., 2012. <em>Bayesian learning for neural networks</em>, Springer Science &amp; Business Media.</p>
</div>
<div id="ref-riedmiller2005neural">
<p>Riedmiller, M., 2005. Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method. In <em>European conference on machine learning</em>. Springer, pp. 317–328.</p>
</div>
<div id="ref-roubtsov2005emma">
<p>Roubtsov, V. &amp; others, 2005. Emma: A free java code coverage tool.</p>
</div>
<div id="ref-rumelhart1985learning">
<p>Rumelhart, D.E., Hinton, G.E. &amp; Williams, R.J., 1985. <em>Learning internal representations by error propagation</em>, DTIC Document.</p>
</div>
<div id="ref-sermanet2013pedestrian">
<p>Sermanet, P. et al., 2013. Pedestrian detection with unsupervised multi-stage feature learning. In <em>Proceedings of the ieee conference on computer vision and pattern recognition</em>. pp. 3626–3633.</p>
</div>
<div id="ref-pmlr-v70-shi17a">
<p>Shi, T. et al., 2017. World of bits: An open-domain platform for web-based agents. In D. Precup &amp; Y. W. Teh, eds. <em>Proceedings of the 34th international conference on machine learning</em>. Proceedings of machine learning research. International Convention Centre, Sydney, Australia: PMLR, pp. 3135–3144. Available at: <a href="http://proceedings.mlr.press/v70/shi17a.html" class="uri">http://proceedings.mlr.press/v70/shi17a.html</a>.</p>
</div>
<div id="ref-silver2016alphago">
<p>Silver, D. &amp; Hassabis, D., 2016. AlphaGo: Mastering the ancient game of go with machine learning. <em>Research Blog</em>.</p>
</div>
<div id="ref-silver2017mastering">
<p>Silver, D. et al., 2017. Mastering the game of go without human knowledge. <em>Nature</em>, 550(7676), pp.354–359.</p>
</div>
<div id="ref-sundermeyer2012lstm">
<p>Sundermeyer, M., Schlüter, R. &amp; Ney, H., 2012. LSTM neural networks for language modeling. In <em>Interspeech</em>. pp. 194–197.</p>
</div>
<div id="ref-sutton1998reinforcement">
<p>Sutton, R.S. &amp; Barto, A.G., 1998. <em>Reinforcement learning: An introduction</em>, MIT press Cambridge.</p>
</div>
<div id="ref-todorov2012mujoco">
<p>Todorov, E., Erez, T. &amp; Tassa, Y., 2012. MuJoCo: A physics engine for model-based control. In <em>Intelligent robots and systems (iros), 2012 ieee/rsj international conference on</em>. IEEE, pp. 5026–5033.</p>
</div>
<div id="ref-tsitsiklis1997analysis">
<p>Tsitsiklis, J.N., Van Roy, B. &amp; others, 1997. An analysis of temporal-difference learning with function approximation. <em>IEEE transactions on automatic control</em>, 42(5), pp.674–690.</p>
</div>
<div id="ref-van2016deep">
<p>Van Hasselt, H., Guez, A. &amp; Silver, D., 2016. Deep reinforcement learning with double q-learning. In <em>AAAI</em>. pp. 2094–2100.</p>
</div>
<div id="ref-werbos1988generalization">
<p>Werbos, P.J., 1988. Generalization of backpropagation with application to a recurrent gas market model. <em>Neural networks</em>, 1(4), pp.339–356.</p>
</div>
<div id="ref-yeh2015covdroid">
<p>Yeh, C.-C. &amp; Huang, S.-K., 2015. CovDroid: A black-box testing coverage system for android. In <em>Computer software and applications conference (compsac), 2015 ieee 39th annual</em>. IEEE, pp. 447–452.</p>
</div>
<div id="ref-zhauniarovich2015towards">
<p>Zhauniarovich, Y. et al., 2015. Towards black box testing of android apps. In <em>Availability, reliability and security (ares), 2015 10th international conference on</em>. IEEE, pp. 501–510.</p>
</div>
</div>
            </body>
</html>

