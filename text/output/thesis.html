<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta http-equiv="Content-Style-Type" content="text/css" />
        <meta name="generator" content="pandoc" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
                        <title></title>
        <style type="text/css">code{white-space: pre;}</style>
                                            <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
                            <style>
            body {
                font-family: Georgia;
                max-width: 800px;
                margin: 0 auto;
                line-height: 30px;
                font-size: 18px;
                padding-left: 350px;
                padding-right: 50px;
                color: #111;
            }
            
            h1, h2, h3, h4, h5, h6 {
                font-family: Arial;
            }
            
            h1 {
                padding-top: 200px;
                line-height: 50px;
            }
            h2 {
                padding-top: 30px;
            }
            h3 {
                padding-top: 20px;
            }
            h4 {
                padding-top: 10px;
            }
            p {
                text-align: justify;
            }
            p a {
                word-wrap: break-word;
                white-space: pre;
            }
            code {
                word-wrap: break-word;
            }
            blockquote {
                border-left: 3px solid #eee;
                margin-left: 20px;
                padding-left: 20px;
            }
            
            ::selection {
                background-color: #E4E4E4;
            }
            
            table {
                width: 100%;
            }
            table caption {
                font-weight: bold;
            }
            table tr {
                padding: 0;
                margin: 0;
                background-color: #f0f0f0;
            }
            table tr.even {
                background-color: #fafafa;
            }
            table td {
                margin: 0;
                padding: 3px 5px;
            }
            
            p span.added {
                color: green;
                background-color: #FFF3C5;
            }
            p span.removed {
                color: red;
                background-color: #FFF3C5;
            }
            
            #title-page {
                padding: 80px 0;
            }
            
            #TOC {
                position: fixed;
                left: 0;
                top: 0;
                overflow-y: scroll;
                height: 100%;
                background: #fafafa;
                max-width: 300px;
                font-family: Arial;
                font-size: 15px;
                line-height: 30px;
            }
            ::-webkit-scrollbar {
                width: 8px;
            }
            ::-webkit-scrollbar-track {
                background-color: #ECECEC;
            }
            ::-webkit-scrollbar-thumb {
                background-color: #B0B0B0;
                border-radius: 8px;
            }
            #TOC > ul {
                padding-right: 10px;
            }
            #TOC ul {
                list-style: none;
                padding-left: 20px;
            }
            #TOC ul li a {
                text-decoration: none;
                color: #364149;
                text-overflow: ellipsis;
                display: block;
                white-space: nowrap;
                overflow: hidden;
            }
            #TOC ul li a:hover {
                color: #008cff;
            }
            
            .figure {
                text-align: center;
            }
            .figure p {
                text-align: center;
                font-style: italic;
            }
            .figure img {
                  width: 100%;
            }
            </style>
                <script src="js/jquery.js"></script>
        <script src="js/diff.js"></script>
        <script src="js/main.js"></script>
    </head>
    <body>
                <!--
                -->
        <div id="title-page">
            <h1>Бейсово Дълбоко Подсилено Самообучение за Автоматично Тестване на Софтуер</h1>
            <h2>Firstname Surname</h2>
        </div>
                    <div id="TOC">
                <ul>
                <li><a href="#абстракт">Абстракт</a></li>
                <li><a href="#благодарности">Благодарности</a></li>
                <li><a href="#list-of-tables">List of tables</a></li>
                <li><a href="#речник">Речник</a></li>
                <li><a href="#увод"><span class="toc-section-number">1</span> Увод</a><ul>
                <li><a href="#структура-на-дисертационния-труд"><span class="toc-section-number">1.1</span> Структура на дисертационния труд</a></li>
                </ul></li>
                <li><a href="#литературен-обзор"><span class="toc-section-number">2</span> Литературен обзор</a><ul>
                <li><a href="#подсилено-обучение"><span class="toc-section-number">2.1</span> Подсилено обучение</a><ul>
                <li><a href="#дълбоко-обучение"><span class="toc-section-number">2.1.1</span> Дълбоко обучение</a></li>
                <li><a href="#дълбоко-подсилено-обучение"><span class="toc-section-number">2.1.2</span> Дълбоко подсилено обучение</a></li>
                </ul></li>
                <li><a href="#бейсова-статистика"><span class="toc-section-number">2.2</span> Бейсова статистика</a><ul>
                <li><a href="#монте-карло-алгоритми-за-марковски-вериги-mcmc"><span class="toc-section-number">2.2.1</span> Монте Карло алгоритми за Марковски Вериги (MCMC)</a></li>
                <li><a href="#извод-със-свободни-вариационни-параметри"><span class="toc-section-number">2.2.2</span> Извод със свободни вариационни параметри</a></li>
                <li><a href="#бейсови-невронни-мрежи"><span class="toc-section-number">2.2.3</span> Бейсови Невронни Мрежи</a></li>
                </ul></li>
                <li><a href="#автоматизирано-тестване-на-гпи"><span class="toc-section-number">2.3</span> Автоматизирано тестване на ГПИ</a><ul>
                <li><a href="#автоматизирано-тестване-на-android-приложения"><span class="toc-section-number">2.3.1</span> Автоматизирано тестване на Android приложения</a></li>
                <li><a href="#проверка-на-качеството"><span class="toc-section-number">2.3.2</span> Проверка на качеството</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#цел-и-задачи"><span class="toc-section-number">3</span> Цел и задачи</a></li>
                <li><a href="#среда-за-тестване-на-android-приложения"><span class="toc-section-number">4</span> Среда за тестване на Android приложения</a></li>
                <li><a href="#генериране-на-входни-данни-за-тестови-случаи"><span class="toc-section-number">5</span> Генериране на входни данни за тестови случаи</a></li>
                <li><a href="#намиране-на-аномалии-в-тестови-случаи"><span class="toc-section-number">6</span> Намиране на аномалии в тестови случаи</a></li>
                <li><a href="#експерименти-и-резултати"><span class="toc-section-number">7</span> Експерименти и резултати</a></li>
                <li><a href="#заключение"><span class="toc-section-number">8</span> Заключение</a><ul>
                <li><a href="#нерешени-проблеми"><span class="toc-section-number">8.1</span> Нерешени проблеми</a></li>
                <li><a href="#бъдеща-работа"><span class="toc-section-number">8.2</span> Бъдеща работа</a></li>
                <li><a href="#дискусия"><span class="toc-section-number">8.3</span> Дискусия</a></li>
                </ul></li>
                <li><a href="#приложение-1-някои-важни-вероятностни-разпределения">Приложение 1: Някои важни вероятностни разпределения</a></li>
                <li><a href="#приложение-2-фигури">Приложение 2: Фигури</a></li>
                <li><a href="#литература">Литература</a></li>
                </ul>
            </div>
                                <!-- 
This is the Latex-heavy title page. 
People outside UCL may want to remove the header logo 
and add the centred logo
-->

<!-- This page is for an official declaration. -->
<p>   </p>
<h1 id="абстракт" class="unnumbered">Абстракт</h1>
<p> </p>
<h1 id="благодарности" class="unnumbered">Благодарности</h1>
<!-- Use the \newpage command to force a new page -->


<p> </p>

<!--
# List of figures {.unnumbered}

For me, this was the only drawback of writing in Markdown: it is not possible to add a short caption to figures and tables. This means that the \listoftables and \listoffigures commands will generate lists using the full titles, which is probably isn't what you want. For now, the solution is to create the lists manually, when everything else is finished.
-->

<p> </p>

<h1 id="list-of-tables" class="unnumbered">List of tables</h1>
<!-- 
For me, this was the only drawback of writing in Markdown: it is not possible to add a short caption to figures and tables. This means that the \listoftables and \listoffigures commands will generate lists using the full titles, which is probably isn't what you want. For now, the solution is to create the lists manually, when everything else is finished.
-->
<p>Table 5.1 This is an example table . . . <br />
Table x.x Short title of the figure . . . </p>
<h1 id="речник" class="unnumbered">Речник</h1>


<p> </p>
<h1 id="увод"><span class="header-section-number">1</span> Увод</h1>
<p>Съществуващите методи не дават възможност за споделяне на наученото от друго приложение, намиране на аномалии във функционалността, бързо научаване на промени (премахване на старо знание) и оценка на несигурността при изпълнение на действие.</p>
<h2 id="структура-на-дисертационния-труд"><span class="header-section-number">1.1</span> Структура на дисертационния труд</h2>
<p><strong>Глава 2</strong> дава познания върху Дълбокото подсилено обучение и Бейсовото моделиране. <strong>Глава 3</strong> поставя целите и задачите на текущата работа. В <strong>Глава 4</strong> се създава среда за тестване на Android мобилни приложения. <strong>Глава 5</strong> представя модел за генериране на входни данни за тестови случаи. В <strong>Глава 6</strong> се представя модел за намиране на аномалии в тестови случай по подадено изображение. Цялостната система е представена в <strong>Глава 7</strong> заедно с емпирични сравнения спрямо други решения. Нерешени проблеми, бъдещи подобрения и дискусия се намират в последната глава.</p>
<h1 id="литературен-обзор"><span class="header-section-number">2</span> Литературен обзор</h1>
<h2 id="подсилено-обучение"><span class="header-section-number">2.1</span> Подсилено обучение</h2>
<p>Един от основните проблеми в сферата на изкуствения интелект е взимане на поредица от решения в стохастична система. Агент, който изучава приложение е пример за такава среда. Този проблем се състой в избира на редица от решения, които да максимизират разгледаните състояния на текущото приложение. Това е по-сложно от задачи в които трябва да се направи само едно решение. Оценката за представянето на агента може да се даде само след много извършени стъпки. Това означава, че той може да избере неправилно действие сега и да разбере за това много по-късно, т.е. имаме <em>забавяне на последствията</em>. Допълнително, не може да наблюдаваме точното състояние на агента, поради липсата на точен модел на приложението, което тества.</p>
<p><strong>Марковски вериги за вземане на решения (MDP)</strong> Моделират системи, които искаме да контролираме. Във всяка времева стъпка <span class="math inline">\(t\)</span>, системата се намира в дадено състояние <span class="math inline">\(s\)</span>. Например, описаният агент може да се намира на даден екран от приложението, след като е натиснал определен бутон. Системата преминава през различни състояния като резултат от действията, които сме избрали. Задачата ни е да избираме действия, които са добри и да минимизираме броя на тези, които не са. Разнообразни проблеми са моделирани чрез MDP формализма. Някои примери са системи за препоръки <span class="citation">(Joachims et al. 1997)</span>, рутиране на мрежи <span class="citation">(Boyan et al. 1994)</span>, управление на асансьори <span class="citation">(Crites &amp; Barto 1996)</span>, навигация на роботи <span class="citation">(Sutton &amp; Barto 1998)</span>.</p>
<p>Подсиленото обучение (RL) <span class="citation">(Sutton &amp; Barto 1998)</span> дава способи за решаване на проблеми, дефинирани чрез MDP формализма. RL агент взаимодейства със среда за определено време. На всяка времева стъпка <span class="math inline">\(t\)</span>, агентът получава състояние <span class="math inline">\(s_t\)</span> и избира действие <span class="math inline">\(a_t\)</span> от пространство с действия <span class="math inline">\(A\)</span>, следвайки политика <span class="math inline">\(\pi(a_t|s_t)\)</span>. Политиката <span class="math inline">\(\pi\)</span> определя поведението на агента. Тя дава функция за преобразуване на състояние <span class="math inline">\(s_t\)</span> до действия <span class="math inline">\(a_t\)</span>. Използвайки дадена политика, агентът получава скаларна награда <span class="math inline">\(r_t\)</span> и преминава в следващо състояние <span class="math inline">\(s_{t + 1}\)</span>, които се определят от функция за наградите <span class="math inline">\(R(s, a)\)</span> и функция даваща вероятности за преминаване в друго състояние <span class="math inline">\(P(s_{t+1}|s_t,a_t)\)</span>. Когато проблемът е дискретен, т.е. може да се разглежда като отделни епизоди, описаният процес продължава докато агента не достигне до крайно състояние и се рестартира. Общата награда, дефинирана като:</p>
<p><span class="math display">\[R_t = \sum_{k=0}^{\infty}\gamma^kr_{t+k}\]</span></p>
<p>представлява обезценена стойност с фактор <span class="math inline">\(\gamma \in (0,1]\)</span>. Агентът се опитва да максимизира очакваната стойност за такава дългосрочна награда във всяко състояние.</p>
<p>Функция на стойностите дава предсказана, обща, бъдеща награда, която измерва до колко добри дадено състояние или двойка състояние-действие са. Стойността на дадено действие <span class="math inline">\(Q^\pi(s, a) = E[R_t|s_t = s, a_t = a]\)</span> ни дава очакваната награда за избиране на дейсвие <span class="math inline">\(a\)</span> в състояние <span class="math inline">\(s\)</span> и следвайки политика <span class="math inline">\(\pi\)</span>. Оптимална стойностна функция <span class="math inline">\(Q^*(s,a)\)</span> предоставя действие <span class="math inline">\(a\)</span>, което максимизира стойността на наградата за дадено състояние <span class="math inline">\(s\)</span>. Може да дефинираме функция даваща стойност на състоянията <span class="math inline">\(V^\pi(s)\)</span>, както и оптималната й версия <span class="math inline">\(V^*(s)\)</span> по сходен начин.</p>
<h3 id="дълбоко-обучение"><span class="header-section-number">2.1.1</span> Дълбоко обучение</h3>
<p>Нека разгледаме един от най-простите статистически модели - линейната регресия <span class="citation">(Gauss 1809; Legendre 1805)</span>. Нека е дадено множество от <span class="math inline">\(N\)</span> входно-изходни двойки <span class="math inline">\(\{(x_1, y_1), ..., (x_n, y_n)\}\)</span>. Например, нека <span class="math inline">\(x\)</span> да е тегло в кг, а <span class="math inline">\(y\)</span> - височина в см на <span class="math inline">\(N\)</span> човека. Линейната регресия прави предположението, че съществува линейна функция, която преобразува всяко <span class="math inline">\(x_i \in \mathbb{R}^Q\)</span> към <span class="math inline">\(y_i \in \mathbb{R}^D\)</span>. Тогава нашият модел е линейна трансформация на входните данни:</p>
<p><span class="math display">\[f(x) = xW + b\]</span></p>
<p>където <span class="math inline">\(W\)</span> е <span class="math inline">\(Q \times D\)</span> матрица и <span class="math inline">\(b\)</span> е вектор от <span class="math inline">\(D\)</span> елемента. Тогава, задачата се свежда до намиране на такива параметри <span class="math inline">\(W\)</span> и <span class="math inline">\(b\)</span>, които минимизират средната квадратична грешка:</p>
<p><span class="math display">\[e = \frac{1}{N}\sum_i||y_i - (x_iW + b)||^2\]</span></p>
<p>В общият случай, връзката между <span class="math inline">\(x\)</span> и <span class="math inline">\(y\)</span> може да не е линейна. Тогава искаме да дефинираме нелинейна функция <span class="math inline">\(f(x)\)</span>, която преобразува входните данни до изходни. За тази цел може да приложим linear basis function regression (превод?) <span class="citation">(Bishop 2007; Gergonne 1815)</span>, където входните данни <span class="math inline">\(x\)</span> се подават на <span class="math inline">\(K\)</span> фиксирани скаларни нелинейни трансформации <span class="math inline">\(\phi_k(x)\)</span> за създаване на свойствен вектор <span class="math inline">\(\Phi(x) = [\phi_1(x), ...,\phi_k(x)]\)</span>. Трансформациите <span class="math inline">\(\phi_k\)</span> наричаме базисни функции. Върху така създаденият вектор се прилага линейна регресия. LBFR може да се сведе до линейна регресия, когато <span class="math inline">\(\phi_k(x) := x_k\)</span> и <span class="math inline">\(K = Q\)</span>. Този тип функции се смятат за фиксирани и взаимно ортогонални. Когато тези ограничения се пропуснат говорим за <em>параметризирани</em> базисни функции.</p>
<h4 id="изкуствени-невронни-мрежи"><span class="header-section-number">2.1.1.1</span> Изкуствени невронни мрежи</h4>
<p>Когато подредим параметризирани базисни функции в йерархия, може да говорим за изкуствени невронни мрежи. Всеки свойствен вектор в тази йерархия ще наричаме слой. Композицията от подобни слоеве в голяма степен води до голямата гъвкавост на тези модели. Често те постигат високи резултати на различни задачи и могат да се приложат върху реални проблеми, работещи върху терабайти от данни.</p>
<p><strong>Feed-forward neural networks.</strong> Нека разгледаме модел с един <em>скрит слой</em> <span class="citation">(Rumelhart et al. 1985)</span>. Нека <span class="math inline">\(x\)</span> е вектор с <span class="math inline">\(Q\)</span> елемента, представящ входните данни. Трансформираме го с афинна трансформация до вектор с <span class="math inline">\(K\)</span> елемента. Отбелязваме с <span class="math inline">\(W_1\)</span> линейната преобразуваща матрица (матрица на теглата) и с <span class="math inline">\(b\)</span> транслацията използвана за трансформиране на <span class="math inline">\(x\)</span> за да получим <span class="math inline">\(xW_1 + b\)</span>. Върху всеки елемент на получената матрица се прилага нелинейна функция <span class="math inline">\(\sigma(\cdot)\)</span>. Резултатът е т. нар. <em>скрит слой</em>, а всеки елемент се нарича <em>мрежова единица</em>. Върху резултатът се прилага втора линейна трансформация с матрица на теглата <span class="math inline">\(W_2\)</span>, която преобразува скрития слой до изходен вектор с <span class="math inline">\(D\)</span> елемента. Имаме <span class="math inline">\(Q \times K\)</span> матрица <span class="math inline">\(W_1\)</span>, <span class="math inline">\(K \times D\)</span> матрица <span class="math inline">\(W_2\)</span> и <span class="math inline">\(b\)</span> - вектор от <span class="math inline">\(K\)</span> елемента. Резултат от дадена невронна мрежа би бил:</p>
<p><span class="math display">\[\hat{y} = \sigma(xW_1 + b)W_2\]</span></p>
<p>при дадени входни данни <span class="math inline">\(x\)</span>.</p>
<p>Когато използваме невронната мрежа за решаване на регресионна задача може да минимизираме Евклидовата грешка:</p>
<p><span class="math display">\[ e^{W_1, W_2, b}(X, Y) = \frac{1}{2N}\sum_{i=1}^{N}||y_i - \hat{y_i}||^2\]</span></p>
<p>където <span class="math inline">\(\{y_1,\dots,y_n\}\)</span> са <span class="math inline">\(N\)</span> наблюдавани изходни стойности, <span class="math inline">\(\{\hat{y_1},\dots,\hat{y_n}\}\)</span> са изходни данни от модела, а <span class="math inline">\(\{x_1,\dots,x_n\}\)</span> са входните данни. Предполагаме, че минимизирайки тази грешка спрямо <span class="math inline">\(W_1, W_2, b\)</span> ще получим модел, който генерализира добре при нови данни <span class="math inline">\(X_{\text{test}}, Y_{\text{test}}\)</span>.</p>
<p>Когато задачата е да се предскаже класа, към който <span class="math inline">\(x\)</span> принадлежи, от множеството <span class="math inline">\(\{1,\dots,D\}\)</span>, използваме същия модел. Промяната се състой в това, че прилагаме softmax функция върху получения резултат. Тази функция ни дава нормализирани оценки за всеки клас:</p>
<p><span class="math display">\[\hat{p_i} = \frac{exp(\hat{y_i})}{\sum d&#39; exp(\hat{y_i&#39;})}\]</span></p>
<p>Когато вземем логаритъма от горната функция, получаваме softmax грешка:</p>
<p><span class="math display">\[ e^{W_1, W_2, b}(X, Y) = -\frac{1}{N}\sum_{i=1}^{N}log(\hat{p}_{i, c_i})\]</span></p>
<p>където <span class="math inline">\(c_i \in \{1, \dots, D\}\)</span> е наблюдавания клас за вход <span class="math inline">\(i\)</span>.</p>
<p>Описаният по-горе модел има проста структура, но може да бъде разширен до по-специализирани такива. Този тип по-сложни модели се използват, когато задачите изискват обработка на поредици или изображения.</p>
<p><strong>Convolutional Neural Networks</strong> CNN е архитектура <span class="citation">(LeCun et al. 1989)</span>, която се използва когато се използват изображения. Задачи, които до скоро се смятаха за нерешими имат решения посредством този тип модели <span class="citation">(Hinton et al. 2012)</span>. Моделът е създаден чрез рекурсивно приложение на конволуции и обединяващи слоеве. Конволуционният слой е линейна трансформация, която запазва пространствена информация от входното изображение.</p>
<p><strong>Recurrent neural networks (RNN)</strong> RNN е модел <span class="citation">(Rumelhart et al. 1985; Werbos 1988)</span>, базиран на поредици от данни, който се използва за обработка на текст, обработка на видео и други <span class="citation">(Kalchbrenner &amp; Blunsom 2013; Sundermeyer et al. 2012)</span>. Входните данни за RNN са поредица от символи. За всяка времева стъпка <span class="math inline">\(t\)</span>, проста невронна мрежа е приложена върху единствен символ, както и изходните данни от мрежата от предишната стъпка.</p>
<p>Конкретно, при дадена редица от входни данни <span class="math inline">\(x = [x_1,\dots,x_t]\)</span> с дължина <span class="math inline">\(T\)</span>, прост RNN модел е създаден чрез повтарящо се приложение на функция <span class="math inline">\(f_h\)</span>. Така се генерира скрито състояние <span class="math inline">\(h_t\)</span> за времева стъпка <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[h_t = f_h(x_t,h_{t-1}) = \sigma(x_tW_h + h_{t-1}U_h + b_h)\]</span></p>
<p>за някаква нелинейна функция <span class="math inline">\(\sigma\)</span>. Изходните данни от модела може да бъдат дефенирани като:</p>
<p><span class="math display">\[\hat{y} = f_y(h_T) = h_TW_y + b_y\]</span></p>
<p>Съществуват и по-сложни RNN модели, като LSTM <span class="citation">(Hochreiter &amp; Schmidhuber 1997)</span> и GRU <span class="citation">(Cho et al. 2014)</span>.</p>
<h3 id="дълбоко-подсилено-обучение"><span class="header-section-number">2.1.2</span> Дълбоко подсилено обучение</h3>
<p>Този тип методи се класифицират, когато използваме дълбоки невронни мрежи за апроксимиране на някой от компонентите на подсиленото обучение: функция на стойностите <span class="math inline">\(V(s;\theta)\)</span>, политика <span class="math inline">\(\pi(a|s;\theta)\)</span> или модела за промяна на състояние и награди. Параметрите <span class="math inline">\(\theta\)</span> представляват тегла в дълбоки невронни мрежи. Когато използваме &quot;плитки&quot; модели, като например линейна регресия, дървета за вземане на решения и др. като апроксиматори на функция, имаме &quot;плитко&quot; подсилено обучение с параметри <span class="math inline">\(\theta\)</span> за съответния модел. Основната разлика между дълбокото и плиткото подсилено обучение се състой в апроксиматора на функцията, която използва. Когато се използва извън политикова апроксимация - например на нелинейни функции, може да се наблюдават нестабилност и разходимост <span class="citation">(Tsitsiklis et al. 1997)</span>. Въпреки това, скорошната работа върху дълбоки <span class="math inline">\(Q\)</span>-мрежи <span class="citation">(Mnih et al. 2015)</span> и <em>AlphaGo</em> <span class="citation">(Silver &amp; Hassabis 2016)</span> стабилизират процеса на обучение и постигат много добри резултати.</p>
<p>Дълбокото подсилено обучение започна рязкото си развитие с работата на <span class="citation">(Mnih et al. 2015)</span>. Преди това, RL даваше нестабилни резултати, когато се използваха нелинейни апроксиматори като невронни мрежи. Дълбоките <span class="math inline">\(Q\)</span> мрежи (DQN) направиха няколко важни приноса: 1) стабилизиране на обучението използвайки дълбоки невронни мрежи <span class="citation">(Lin 1992)</span> 2) подход за цялостно обучение без почти никакво познание за областта 3) обучаване на гъвкава невронна мрежа с еднакъв алгоритъм за изпълняване на различни задачи, например 49 Atari игри <span class="citation">(Bellemare et al. 2013)</span> на които се представят по-добре от всеки известен алгоритъм до момента.</p>
<h4 id="double-dqn"><span class="header-section-number">2.1.2.1</span> Double DQN</h4>
<p><span class="citation">(Van Hasselt et al. 2016)</span> предложиха Double DQN (D-DQN) за справяне с проблема на прекалена увереност (overestimate?) на Q-learning алгоритъма. В базовият алгоритъм (както и в DQN), параметрите се обновяват според:</p>
<p><span class="math display">\[\theta_{t + 1} = \theta_t + \alpha(y_t^{\theta} - Q(s_t, a_t; \theta_t))\Delta_{\theta_t}Q(s_t,a_t;\theta_{t})\]</span></p>
<p>където</p>
<p><span class="math display">\[y_t^Q = r_{t + 1} + \gamma\max\limits_{\alpha}Q(s_{t+1},a;\theta_t)\]</span></p>
<p>така че оператора <span class="math inline">\(\max\)</span> използва еднакви стойности за да избере и оцени дадено действие. Като следствие от това е по-вероятно да избере недостатъчно добри стойности. Double DQN предлага да оцени алчната политика спрямо невронна мрежа, но използва друга за да оцени стойността й. Това може да се постигне с малка промяна на DQN алгоритъма, заменяме <span class="math inline">\(y_t^Q\)</span> с:</p>
<p><span class="math display">\[y_t^{D - DQN} = r_{t +1} + \gamma Q(s_{t+1},\max\limits_{\alpha}Q(s_{t+1},a_t;\theta_t);\theta_{\bar{t}})\]</span></p>
<p>където <span class="math inline">\(\theta_t\)</span> е параметър за първата невронна мрежа, а <span class="math inline">\(\theta_{\bar{t}}\)</span> е параметър за целевата мрежа.</p>
<h4 id="асинхронни-методи"><span class="header-section-number">2.1.2.2</span> Асинхронни методи</h4>
<p><span class="citation">(Mnih et al. 2016)</span> предложи асинхронни методи за четири RL алгоритъма: Q-learning, SARSA, <span class="math inline">\(n\)</span>-step Q-learning and advantage actor-critic и asynchronous advantage actor-critic (A3C). Този подход използва паралелни агенти, които използват различни политики за изучаване на средата. Асинхронните методи могат да се изпълняват върху многоядрени процесори. Те се изпълняват много по-бързо и предоставят по-бързо обучение от други известни методи.</p>
<h2 id="бейсова-статистика"><span class="header-section-number">2.2</span> Бейсова статистика</h2>
<p>Избиране на следващо действие по време на създаване на тестов случай пряко зависи от увереността във взимането на правилното решение. Несигурността от избиране на действие може да бъде моделирана посредством Бейсов подход.</p>
<p>Нека <span class="math inline">\(\theta\)</span> е неизвестна стойност, която може да е скаларна, вектор или матрица. Методите за статистически извод (inference) могат да ни помогнат да я намерим. Класическият статистически подход третира <span class="math inline">\(\theta\)</span> като фиксирана стойност. Единствената информация, която използваме за намиране на неизвестната стойност, идва от данните с които разполагаме. Изводът се базира на резултат получен от фунцкията на правдоподобие на <span class="math inline">\(\theta\)</span>, която свързва стойности от <span class="math inline">\(p(y|\theta)\)</span> с всяка възможност на <span class="math inline">\(\theta\)</span>, където <span class="math inline">\(y = (y_1,...,y_n)\)</span> е вектор с наблюдавани стойности.</p>
<p>Бейсовият подход третира <span class="math inline">\(\theta\)</span> като случайна стойност. За достигане на извод се използва разпределението на параметри при дадени данни <span class="math inline">\(p(\theta|y)\)</span>. Това разпределение се нарича апостериорно. Освен функцията на правдоподобие, Бейсовият подход включва априорно разпределение <span class="math inline">\(p(\theta)\)</span>, което представя вярванията ни за <span class="math inline">\(\theta\)</span> преди да се разгледат данните.</p>
<p>Теоремата на Бейс дава връзка между фунцкията на правдоподобие и априорното разпределение:</p>
<p><span class="math display">\[p(\theta|y) = \frac{p(\theta|y)p(\theta)}{p(y)}\]</span></p>
<p>където:</p>
<p><span class="math display">\[p(y) = \int p(y|\theta)p(\theta)d\theta\]</span></p>
<p>Формулата на Бейс може да бъде пренаписана по следния начин:</p>
<ol class="example" style="list-style-type: decimal">
<li><span class="math display">\[p(\theta|y) \propto p(\theta|y)p(\theta)\]</span></li>
</ol>
<p>тъй като <span class="math inline">\(p(y)\)</span> не зависи от <span class="math inline">\(\theta\)</span></p>
<p>Когато <span class="math inline">\(\theta\)</span> е многомерна величина може да напишем уравнение (1) използвайки маргиналните апостериорни разпределения като например:</p>
<p><span class="math display">\[p(\theta_1|y) = \int p(\theta|y)d\theta_2\]</span></p>
<p>където <span class="math inline">\(\theta = (\theta_1, \theta_2)\)</span>. В много случаи резултатите са многомерни и точни изводи може да бъдат направени само аналитично. Поради тази причина често се използват приближения.</p>
<h3 id="монте-карло-алгоритми-за-марковски-вериги-mcmc"><span class="header-section-number">2.2.1</span> Монте Карло алгоритми за Марковски Вериги (MCMC)</h3>
<p>MCMC алгоритмите правят неявно интегриране като взимат извадки от апостериорното разпределение. По този начин се намират приближения на стойностите от които се интересуваме.</p>
<p>В съществото си тези методи създават Марковска верига с апостериорното разпределение на параметрите като стационарно разпределение. Когато веригата е крайна и повтаряща се, стойността на <span class="math inline">\(\theta\)</span> може да бъде оценена от извадки на средни пътища. Генерираните извадки <span class="math inline">\(\theta^{(t)}, t=1, \ldots, N\)</span> от това разпределение дават представа за целевото разпределение.</p>
<h4 id="метрополис-хастингс-алгоритъм"><span class="header-section-number">2.2.1.1</span> Метрополис-Хастингс алгоритъм</h4>
<p>Този алгоритъм е предложен от Metropolis <span class="citation">(Metropolis et al. 1953)</span> и по-късно генерализиран от Hastings <span class="citation">(Hastings 1970)</span>. Методът създава Марковска верига с желаното стационарно разпределение. Алгоритъмът избира кандидат стойност <span class="math inline">\(\theta&#39;\)</span> от предварително избрано разпределение <span class="math inline">\(q(\theta, \theta&#39;)\)</span>, където <span class="math inline">\(\theta&#39; \neq \theta\)</span>. Избраната стойност <span class="math inline">\(\theta&#39;\)</span> се проверява чрез приеми-откажи метод (accept-reject step) за да се подсигури, че принадлежи на целевото разпределение.</p>
<h4 id="извадки-на-гибс"><span class="header-section-number">2.2.1.2</span> Извадки на Гибс</h4>
<p>Този метод, предложен от Geman и Geman <span class="citation">(Geman &amp; Geman 1984)</span>, често се представя като специален случай на Метрополис-Хастингс алгоритъма.</p>
<h3 id="извод-със-свободни-вариационни-параметри"><span class="header-section-number">2.2.2</span> Извод със свободни вариационни параметри</h3>
<p>Variational Inference (VI) методите обикновено предлагат по-добри резултати спрямо MCMC, когато времето за изпълнение е ограничено. Допълнително предимство на тези подходи е, че те са детерминирани. Систематичната грешка и дисперсията се приближават до 0 при MCMC методите, за колкото повече време бъдат оставени да се изпълняват те. Тези свойства правят MCMC алгоритмите много ефективни на теория. В практиката обаче, времето за изпълнение и изчислителната мощ са ограничени. Това налага търсенето на по-бързо методи, дори когато това намаля точността на получените резултати.</p>
<p>Този тип методи дефинират приближено вариационно разпределение <span class="math inline">\(q_\omega(\theta)\)</span>, параметризирано от <span class="math inline">\(\omega\)</span>, с лесна за оценяване структура. Искаме приближеното разпределение да е максимално близко до това на апостериорното. За целта свеждаме задачата до оптимизационна и минимизираме Kullback-Leibler (KL) <span class="citation">(Kullback &amp; Leibler 1951)</span> отклонението спрямо <span class="math inline">\(\omega\)</span>. Интуитивно, KL измерва приликата между две разпределения:</p>
<p><span class="math display">\[KL(q\omega(\theta)\,||\,p(\theta|x, y)) = \int q\omega(\theta)log\frac{q\omega(\theta)}{p(\omega|x, y)}d\omega\]</span></p>
<p>(Define x,y - dataset)</p>
<p>Този интеграл е дефиниран когато <span class="math inline">\(q\omega(\theta)\)</span> е непрекъсната спрямо <span class="math inline">\(p(\theta|x, y)\)</span>. Нека <span class="math inline">\(q^*_\omega(\theta)\)</span> е минимизираща точка (може да е локален минимум). Тогава KL може да ни даде приближение на апостериорното разпределение:</p>
<p><span class="math display">\[p(y^*|x^*, x, y) \approx \int p(y^*|x^*, \theta)q^*_\omega(\theta)d\theta =: q^*_\omega(y^*|x^*)\]</span></p>
<p>VI методите заменят изчисляването на интеграли с такова на производни. Това е много подобно на оптимизационните методи използвани в DL. Основната разлика се състой в това, че оптимизацията е върху разпределения, а не точкови оценки. Този подход запазва много от предимствата на Бейсовото моделиране и представя вероятностни модели, които дават оценка на несигурността в изводите си.</p>
<h3 id="бейсови-невронни-мрежи"><span class="header-section-number">2.2.3</span> Бейсови Невронни Мрежи</h3>
<p>Един от големите недостатъци на съществуващите архитектури на невронни мрежи е, че изводите, които получаваме, са оценки на точки. Моделите не казват до колко са сигурни в предложените резултати. Когато например един лекар получи резултат от даден модел, той трябва да знае защо и как модела е стигнал до него. Бейсовата статистика може да даде отговор на тези въпроси <span class="citation">(Gal &amp; Ghahramani 2015)</span>. Дори при модели използващи RNN, Бейсова интерпретация на задачата дава по-добри резултати от съществуващи такива <span class="citation">(Gal &amp; Ghahramani 2016)</span>.</p>
<p>Бейсови невронни мрежи, предложени в края на 80-те години <span class="citation">(Kononenko 1989)</span> и задълбочено изучавани по-късно <span class="citation">(MacKay 1992; Neal 2012)</span>, предлагат вероятностна интерпретация на моделите за дълбоко обучение, като представят теглата им като вероятностни разпределения. Този тип модели са устойчиви на пренастройване (overfitting), предлагат оценки на несигурността и могат да се тренират върху малко на брой данни.</p>
<h2 id="автоматизирано-тестване-на-гпи"><span class="header-section-number">2.3</span> Автоматизирано тестване на ГПИ</h2>
<p>Проверката за правилно поведение на софтуер продукт е неизменна част от създаването му. Откриване и поправяне на всички потенциални проблеми преди той да бъде доставен до крайния потребител може да се сметне за най-добър случай.</p>
<h3 id="автоматизирано-тестване-на-android-приложения"><span class="header-section-number">2.3.1</span> Автоматизирано тестване на Android приложения</h3>
<p>Мобилните приложения също имат нужда от проверка на качеството. Поради тази причина, в последните години засилено се разглеждат начини за автоматизацията на подобен вид тестове. Много голяма част от извършената работа до момента се състои в създаване на входни данни за приложения за мобилната операционна система Android. Подходите използвани до момента се различават по начина по който създават входнни данни, изучават и използват евристики за приложението.</p>
<h4 id="съществуващи-системи"><span class="header-section-number">2.3.1.1</span> Съществуващи системи</h4>
<p><strong>Dynodroid</strong> <span class="citation">(Machiry et al. 2013)</span> е инструмент, който се базира на случайно изучаване. Предлага се и ръчен начин за въвеждане на входнни данни, когато системата е заседнала.</p>
<p><strong>MobiGUITAR</strong> <span class="citation">(Amalfitano et al. 2015)</span> строй модел на приложението по време на тестване. За всяко ново състояние се поддържа списък с възможни действия, които се изпълняват използвайки DFS (depth first search) стратегия.</p>
<p><strong>SwiftHand</strong> <span class="citation">(Choi et al. 2013)</span> се опитва да максимизира покритието на код за тестваното приложение. Допълнително, инструментът се старае да минимизира броя рестартирания на приложението. SwiftHand генерира единствено докосвания и скролвания.</p>
<p><strong>PUMA</strong> <span class="citation">(Hao et al. 2014)</span> предлага генерална среда за автоматизиране на ГПИ. Инструментът предлага рамка за програмиране в която могат да бъдат имплементирани различни стратегии за изучаване на тестваното приложение.</p>
<h4 id="покритие-на-код"><span class="header-section-number">2.3.1.2</span> Покритие на код</h4>
<p>BBoxTester: <span class="citation">(Zhauniarovich et al. 2015)</span> CovDroid: <span class="citation">(Yeh &amp; Huang 2015)</span></p>
<h4 id="текущо-състояние-state-of-the-art"><span class="header-section-number">2.3.1.3</span> Текущо състояние (State of the art?)</h4>
<p><span class="citation">(Choudhary et al. 2015)</span></p>
<h3 id="проверка-на-качеството"><span class="header-section-number">2.3.2</span> Проверка на качеството</h3>
<ul>
<li>Достатъчно бързо ли е? (Model should monitor for speed exec anomalies or report just slow parts)</li>
<li>Как да повторя грешката? (Provide/execute steps for reproduction)</li>
<li>Има ли разлики в изходните данни? (Change in hierarchy/image screenshot)</li>
</ul>
<h1 id="цел-и-задачи"><span class="header-section-number">3</span> Цел и задачи</h1>
<p>Целта на настоящата дисертацаионна работа е да създаде система за автоматизирано тестване на ГПИ, която използва за входни данни само визуалния изход на тестваното приложение. За постигане на целта трябва да се изпълнят следните задачи:</p>
<ul>
<li>Избор на подходящи оценъчни функции и награди, които да мотивират максималното покритие на програмен код по време на тестване</li>
<li>Създаване на структури, в които да се запазват поредиците от стъпки, необходими за повтаряне на тестови случай</li>
<li>Създаване на модел, който генерира поредица от действия използвани за играждане на тестовите случаи</li>
<li>Създаване на модел, който намира аномалии по време на изпълнение на програмата</li>
<li>Автоматично именуване на отделни екрани и действия с цел улесняване на разбирането</li>
<li>Създаване и провеждане на експерименти, които да сравнят преложения модел с вече съществуващи такива</li>
<li>По подадено изображение, йерархия на изгледите и действия да се определи кои действия са валидни върху кои елементи</li>
</ul>
<h1 id="среда-за-тестване-на-android-приложения"><span class="header-section-number">4</span> Среда за тестване на Android приложения</h1>
<h1 id="генериране-на-входни-данни-за-тестови-случаи"><span class="header-section-number">5</span> Генериране на входни данни за тестови случаи</h1>
<h1 id="намиране-на-аномалии-в-тестови-случаи"><span class="header-section-number">6</span> Намиране на аномалии в тестови случаи</h1>
<h1 id="експерименти-и-резултати"><span class="header-section-number">7</span> Експерименти и резултати</h1>
<h1 id="заключение"><span class="header-section-number">8</span> Заключение</h1>
<h2 id="нерешени-проблеми"><span class="header-section-number">8.1</span> Нерешени проблеми</h2>
<h2 id="бъдеща-работа"><span class="header-section-number">8.2</span> Бъдеща работа</h2>
<h2 id="дискусия"><span class="header-section-number">8.3</span> Дискусия</h2>
<h1 id="приложение-1-някои-важни-вероятностни-разпределения" class="unnumbered">Приложение 1: Някои важни вероятностни разпределения</h1>
<!-- 
This could be a list of papers by the author for example 
-->
<h1 id="приложение-2-фигури" class="unnumbered">Приложение 2: Фигури</h1>
<!-- 
This could include extra figures or raw data
-->

<!-- 
Do not edit this page.

References are automatically generated from the BibTex file (References.bib)

...which you should create using your reference manager.
-->
<h1 id="литература" class="unnumbered">Литература</h1>
<div id="refs" class="references">
<div id="ref-amalfitano2015mobiguitar">
<p>Amalfitano, D. et al., 2015. MobiGUITAR: Automated model-based testing of mobile apps. <em>IEEE Software</em>, 32(5), pp.53–59.</p>
</div>
<div id="ref-bellemare2013arcade">
<p>Bellemare, M.G. et al., 2013. The arcade learning environment: An evaluation platform for general agents. <em>J. Artif. Intell. Res.(JAIR)</em>, 47, pp.253–279.</p>
</div>
<div id="ref-bishop2007pattern">
<p>Bishop, C., 2007. Pattern recognition and machine learning (information science and statistics), 1st edn. 2006. corr. 2nd printing edn. <em>Springer, New York</em>.</p>
</div>
<div id="ref-boyan1994packet">
<p>Boyan, J.A., Littman, M.L. &amp; others, 1994. Packet routing in dynamically changing networks: A reinforcement learning approach. <em>Advances in neural information processing systems</em>, pp.671–671.</p>
</div>
<div id="ref-cho2014learning">
<p>Cho, K. et al., 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. <em>arXiv preprint arXiv:1406.1078</em>.</p>
</div>
<div id="ref-choi2013guided">
<p>Choi, W., Necula, G. &amp; Sen, K., 2013. Guided gui testing of android apps with minimal restart and approximate learning. In <em>ACM sigplan notices</em>. ACM, pp. 623–640.</p>
</div>
<div id="ref-choudhary2015automated">
<p>Choudhary, S.R., Gorla, A. &amp; Orso, A., 2015. Automated test input generation for android: Are we there yet?(e). In <em>Automated software engineering (ase), 2015 30th ieee/acm international conference on</em>. IEEE, pp. 429–440.</p>
</div>
<div id="ref-crites1996improving">
<p>Crites, R.H. &amp; Barto, A.G., 1996. Improving elevator performance using reinforcement learning. <em>Advances in neural information processing systems</em>, 8.</p>
</div>
<div id="ref-gal2016theoretically">
<p>Gal, Y. &amp; Ghahramani, Z., 2016. A theoretically grounded application of dropout in recurrent neural networks. In <em>Advances in neural information processing systems</em>. pp. 1019–1027.</p>
</div>
<div id="ref-gal2015dropout">
<p>Gal, Y. &amp; Ghahramani, Z., 2015. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. <em>arXiv preprint arXiv:1506.02142</em>, 2.</p>
</div>
<div id="ref-gauss1809theoria">
<p>Gauss, C.F., 1809. <em>Theoria motus corporum coelestium in sectionibus conicis solem ambientium auctore carolo friderico gauss</em>, sumtibus Frid. Perthes et IH Besser.</p>
</div>
<div id="ref-geman1984stochastic">
<p>Geman, S. &amp; Geman, D., 1984. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. <em>IEEE Transactions on pattern analysis and machine intelligence</em>, (6), pp.721–741.</p>
</div>
<div id="ref-gergonne1815application">
<p>Gergonne, J., 1815. Application de la méthode des moindres quarrésa l’interpolation des suites. <em>Annales de Math. Pures et Appl</em>, 6, pp.242–252.</p>
</div>
<div id="ref-hao2014puma">
<p>Hao, S. et al., 2014. Puma: Programmable ui-automation for large-scale dynamic analysis of mobile apps. In <em>Proceedings of the 12th annual international conference on mobile systems, applications, and services</em>. ACM, pp. 204–217.</p>
</div>
<div id="ref-hastings1970monte">
<p>Hastings, W.K., 1970. Monte carlo sampling methods using markov chains and their applications. <em>Biometrika</em>, 57(1), pp.97–109.</p>
</div>
<div id="ref-hinton2012improving">
<p>Hinton, G.E. et al., 2012. Improving neural networks by preventing co-adaptation of feature detectors. <em>arXiv preprint arXiv:1207.0580</em>.</p>
</div>
<div id="ref-hochreiter1997long">
<p>Hochreiter, S. &amp; Schmidhuber, J., 1997. Long short-term memory. <em>Neural computation</em>, 9(8), pp.1735–1780.</p>
</div>
<div id="ref-joachims1997webwatcher">
<p>Joachims, T. et al., 1997. Webwatcher: A tour guide for the world wide web. In <em>IJCAI (1)</em>. Citeseer, pp. 770–777.</p>
</div>
<div id="ref-kalchbrenner2013recurrent">
<p>Kalchbrenner, N. &amp; Blunsom, P., 2013. Recurrent continuous translation models. In <em>EMNLP</em>. p. 413.</p>
</div>
<div id="ref-kononenko1989bayesian">
<p>Kononenko, I., 1989. Bayesian neural networks. <em>Biological Cybernetics</em>, 61(5), pp.361–370.</p>
</div>
<div id="ref-kullback1951information">
<p>Kullback, S. &amp; Leibler, R.A., 1951. On information and sufficiency. <em>The annals of mathematical statistics</em>, 22(1), pp.79–86.</p>
</div>
<div id="ref-lecun1989backpropagation">
<p>LeCun, Y. et al., 1989. Backpropagation applied to handwritten zip code recognition. <em>Neural computation</em>, 1(4), pp.541–551.</p>
</div>
<div id="ref-legendre1805nouvelles">
<p>Legendre, A.M., 1805. <em>Nouvelles méthodes pour la détermination des orbites des comètes</em>, F. Didot.</p>
</div>
<div id="ref-lin1992self">
<p>Lin, L.-J., 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. <em>Machine learning</em>, 8(3-4), pp.293–321.</p>
</div>
<div id="ref-machiry2013dynodroid">
<p>Machiry, A., Tahiliani, R. &amp; Naik, M., 2013. Dynodroid: An input generation system for android apps. In <em>Proceedings of the 2013 9th joint meeting on foundations of software engineering</em>. ACM, pp. 224–234.</p>
</div>
<div id="ref-mackay1992practical">
<p>MacKay, D.J., 1992. A practical bayesian framework for backpropagation networks. <em>Neural computation</em>, 4(3), pp.448–472.</p>
</div>
<div id="ref-metropolis1953equation">
<p>Metropolis, N. et al., 1953. Equation of state calculations by fast computing machines. <em>The journal of chemical physics</em>, 21(6), pp.1087–1092.</p>
</div>
<div id="ref-mnih2016asynchronous">
<p>Mnih, V. et al., 2016. Asynchronous methods for deep reinforcement learning. In <em>International conference on machine learning</em>.</p>
</div>
<div id="ref-mnih2015human">
<p>Mnih, V. et al., 2015. Human-level control through deep reinforcement learning. <em>Nature</em>, 518(7540), pp.529–533.</p>
</div>
<div id="ref-neal2012bayesian">
<p>Neal, R.M., 2012. <em>Bayesian learning for neural networks</em>, Springer Science &amp; Business Media.</p>
</div>
<div id="ref-rumelhart1985learning">
<p>Rumelhart, D.E., Hinton, G.E. &amp; Williams, R.J., 1985. <em>Learning internal representations by error propagation</em>, DTIC Document.</p>
</div>
<div id="ref-silver2016alphago">
<p>Silver, D. &amp; Hassabis, D., 2016. AlphaGo: Mastering the ancient game of go with machine learning. <em>Research Blog</em>.</p>
</div>
<div id="ref-sundermeyer2012lstm">
<p>Sundermeyer, M., Schlüter, R. &amp; Ney, H., 2012. LSTM neural networks for language modeling. In <em>Interspeech</em>. pp. 194–197.</p>
</div>
<div id="ref-sutton1998reinforcement">
<p>Sutton, R.S. &amp; Barto, A.G., 1998. <em>Reinforcement learning: An introduction</em>, MIT press Cambridge.</p>
</div>
<div id="ref-tsitsiklis1997analysis">
<p>Tsitsiklis, J.N., Van Roy, B. &amp; others, 1997. An analysis of temporal-difference learning with function approximation. <em>IEEE transactions on automatic control</em>, 42(5), pp.674–690.</p>
</div>
<div id="ref-van2016deep">
<p>Van Hasselt, H., Guez, A. &amp; Silver, D., 2016. Deep reinforcement learning with double q-learning. In <em>AAAI</em>. pp. 2094–2100.</p>
</div>
<div id="ref-werbos1988generalization">
<p>Werbos, P.J., 1988. Generalization of backpropagation with application to a recurrent gas market model. <em>Neural networks</em>, 1(4), pp.339–356.</p>
</div>
<div id="ref-yeh2015covdroid">
<p>Yeh, C.-C. &amp; Huang, S.-K., 2015. CovDroid: A black-box testing coverage system for android. In <em>Computer software and applications conference (compsac), 2015 ieee 39th annual</em>. IEEE, pp. 447–452.</p>
</div>
<div id="ref-zhauniarovich2015towards">
<p>Zhauniarovich, Y. et al., 2015. Towards black box testing of android apps. In <em>Availability, reliability and security (ares), 2015 10th international conference on</em>. IEEE, pp. 501–510.</p>
</div>
</div>
            </body>
</html>

